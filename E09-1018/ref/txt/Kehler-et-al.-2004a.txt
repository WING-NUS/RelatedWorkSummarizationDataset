competitive self-trained pronoun interpretation .
abstract .
we describe a system for pronoun interpretation that is self-trained from raw data , that is , using no annotated training data .
the result outperforms a hobbsian baseline algorithm and is only marginally inferior to an essentially identical , state-of-the-art supervised model trained from a substantial manually-annotated coreference corpus .
introduction .
the last several years have seen a number of feature- based systems for pronoun interpretation in which the feature weights are determined via manual experimentation or supervised learning ( see mitkov ( 2002 ) for a useful survey ) .
reliable estimation of the weights in both paradigms requires a substantial manually-annotated corpus of examples .
in this short paper we describe a system for ( third-person ) pronoun interpretation that is self-trained from raw data , that is , using no annotated training data whatsoever .
the result outperforms a hobbsian baseline algorithm and is only marginally inferior ( 2.3 % ) to an essentially identical , state-of-the-art supervised model trained from a manually-annotated coreference corpus .
this result leaves open the possibility that systems self-trained on very large datasets with more finely-grained features could eventually outperform supervised models that rely on manually-annotated datasets .
the remainder of the paper is organized as follows .
we first briefly describe the supervised system ( described in more detail in kehler et al. ( 2004 ) ) to which we will compare the self-trained system .
both systems use the same learning algorithm and feature set ; they differ with respect to whether the data they are trained on is annotated by a human or the algorithm itself .
we then describe our hobbsian baseline algorithm , and present the results of all three systems .
the supervised algorithm .
the supervised model was trained using the improved iterative scaling algorithm for maximum entropy ( maxent ) models described by berger et al. ( 1996 ) with binary-valued features .
as is standard , the model was trained as a binary coreference classifier : for each possible antecedent of each pronoun , a training instance was created that consisted of the pronoun , the possible antecedent phrase , and a binary coreference outcome . ( such a model can be seen as providing a probabilistic measure of antecedent salience . )
because we are ultimately interested in identifying the correct antecedent among a set of possible ones , during testing the antecedent assigned the highest probability is chosen .
the algorithm receives as input the results of sris textpro system , a shallow parser that recognizes low-level constituents ( noun groups , verb groups , etc . ) .
no difficult syntactic attachments are attempted , and the results are errorful .
there was no human-annotated linguistic information in the input .
the training corpus consists of 2773 annotated third-person pronouns from the newspaper and newswire segments of the automatic content extraction ( ace ) program training corpus .
the annotated blind corpus used for evaluation consists of 762 annotated third-person pronouns from the ace february 2002 evaluation set .
the annotated pronouns in both sets include only those that are ace markables , i.e. , ones that refer to entities of the following types : persons , organizations , geopoliticalentities ( politically defined geographical regions , their governments , or their people ) , locations , and facilities .
the system employs a set of hard constraints and soft features .
the hard constraints filter out those noun groups that fail conservative number and gender agreement checks before training , whereas the soft features are used by the maxent algorithm .
a set of forty soft features were developed and optimized manually ; they fall into five categories that have become fairly standard in the literature : gender agreement : includes features to test a strict match of gender ( e.g. , a masculine pronoun and a masculine antecedent ) , as well as mere compatibility ( e.g. , a masculine pronoun with an antecedent of unknown gender ) .
these features are more liberal than the gender-based hard constraint mentioned above .
number agreement : includes features to test a strict match of number ( e.g. , a singular pronoun and a singular antecedent ) , as well as mere compatibility ( e.g. , a singular pronoun with an antecedent of unknown number ) .
these features are likewise more liberal than the number-based hard constraint mentioned above .
distance : includes features pertaining to the distance between the pronoun and the potential antecedent .
examples include the number of sentences between them and the hobbs distance , that is , the number of noun groups that have to be skipped before the potential antecedent is found per the search order used by the hobbs algorithm ( hobbs , 1978 ; ge et al. , 1998 ) .
grammatical role : includes features pertaining to the syntactic position of the potential antecedent .
examples include whether the potential antecedent appears to be the subject or object of a verb , and whether the potential antecedent is embedded in a prepositional phrase .
linguistic form : includes features pertaining to the referential form of the potential antecedent , e.g. , whether it is a proper name , definite description , indefinite np , or a pronoun .
the values of these features computed from textpros errorful shallow constituent parses comprised the input to the learning algorithm , along with the outcome as indicated by the annotated key .
the self-trained algorithm .
the self-trained algorithm likewise uses maxent , with the same feature set and shallow parser .
the two systems differ in the training data utilized .
instead of the training corpus of 2773 annotated pronouns used in the supervised experiments , the self-trained algorithm creates training data from pronouns found in a raw corpus , particularly the newswire segment of the topic detection and tracking ( tdt-2 ) corpus .
the system was evaluated on the same annotated set of 762 pronouns as the supervised system ; the performance statistics reported herein are from the only time an evaluation with this data was carried out .
the self-trained system embeds the maxent algorithm in an iterative loop during which the training examples are acquired .
the first phase of the algorithm builds an initial model as follows : for each third-person pronoun : collect possible antecedents , that is , all of the noun groups found in the previous two sentences and to the left of the pronoun in the current sentence .
filter them by applying the hard constraints .
if only one possible antecedent remains , create a pronoun-antecedent pair and label the coreference outcome as true .
otherwise , with some probability ( 0.2 in our experiments ' ) , create a pronoun-antecedent pair for each possible antecedent and label the coreference outcome as false .
train a maxent classifier on this training data .
the simplification assumed above that coreference holds for all and only those pronouns for which textpro and hard constraints find a single possible antecedent is obviously false , but it nonetheless yields a model to seed the iterative part of the algorithm , which goes as follows : for each pronoun in the training data acquired in step 1 : apply the current maxent model to each pronoun-antecedent pair .
label the pair to which the model assigns the highest probability the coreference outcome of true .
label all other pairs ( if any ) for that pronoun the outcome of false .
retrain the maxent model with this new training data .
repeat steps 3 and 4 until the training data reaches a steady state , that is , there are no pronouns for which the current model changes its preference to a different potential antecedent than it favored during the previous iteration .
the hope is that improved predictions about which potential antecedents of ambiguous pronouns are correct will yield iteratively better models ( note that the unambiguous pronoun-antecedent pairs collected in step 1c will be considered to be correct throughout ) .
this hope is notwithstanding the fact that the algorithm is based on a simplifying assumption that each pronoun is associated with exactly one correct antecedent that is clearly false for a variety of reasons : ( i ) there will be cases in which there is more than one coreferential antecedent in the search window , all but one of which will get labeled as not coreferential during any given iteration , ( ii ) there will be cases in which the ( perhaps only ) correct antecedent was misparsed or incorrectly weeded out by hard constraints , and thus not seen by the learning algorithm ( presumably some of the unambiguous cases identified in step 1c will be incorrect because of this ) , and ( iii ) some of the pronouns found will not even be referential , e.g. pleonastic pronouns .
the empirical question remains , however , of how good of a system can be trained under such an assumption .
after all , the model probabilities need not necessarily be accurate in an absolute sense , but only in a relative one : that is , good enough so that the antecedent assigned the highest probability tends to be correct .
hobbs baseline .
for comparison purposes , we also implemented a version of hobbss ( 1978 ) well-known pronoun interpretation algorithm , in which no machine learning is involved .
this algorithm takes the syntactic representations of the sentences up to and including the current sentence as input , and performs a search for an antecedent noun phrase on these trees .
since textpro does not build full syntactic trees for the input , we developed a version that does a simple search through the list of noun groups recognized .
in accordance with hobbss search procedure , noun groups are searched in the following order : ( i ) in the current sentence from right-to-left , starting with the first noun group to the left of the pronoun , ( ii ) in the previous sentence from left-to-right , ( iii ) in two sentences prior from left-to-right , ( iv ) in the current sentence from left-to-right , starting with the first noun group to the right of the pronoun ( for cataphora ) .
the first noun group encountered that agrees with the pronoun with respect to number , gender , and person is chosen as the antecedent .
results .
reporting on the results of a self-trained system means only evaluating the system against annotated data once , since any system reconfiguration and reevaluation based on the feedback received would constitute a form of indirectly supervised training .
thus we had to select a configuration as representing our reportable system before doing any evaluation .
to allow for the closest comparison with our supervised system , we opted to train the system with the same number of pronouns that we had in our supervised training set ( 2773 ) , and sought to have approximately the same ratio of positive to negative training instances , which meant randomly including one-fifth of the pronouns in the raw data that had more than one possible antecedent ( see step 1d ) .
later we report on post-hoc experiments to assess the effect of training data size on performance .
the self-trained system was trained fourteen times , once using each of fourteen different segments of the tdt-2 data that we had arbitrarily apportioned at the inception of the project .
the scores reported below and in table 1 for the self-trained system are averages of the fourteen corresponding evaluations .
the final results are as follows : the self-trained system beats the competitive hobbs baseline system by 4.6 % and comes within 2.3 % of the supervised system trained on the same number of manually-annotated pronouns .
convergence for the self-trained system was fairly rapid , taking between 8 and 14 iterations .
the number of changes in the current models predictions started off relatively high in early iterations ( averaging approximately 305 pronouns or 11 % of the dataset ) and then steadily declined ( usually , but not always , monotonically ) until convergence .
post-hoc analysis showed that the iterative phase contributed a gradual ( although again not completely monotonic ) improvement in performance during the course of learning .
we then performed a set of post-hoc experiments to measure the effect of training data size on performance for the self-trained system .
the results are given in table 1 , which show a gradual increase in performance as the number of pronouns grows .
the final row includes the results when all of the unambiguous pronouns in each tdt segment are utilized ( again , along with approximately one-fifth of the ambiguous pronouns ) , which amounted to between 7,212 and 11,245 total pronouns.3 ( note that since most pronouns have more than one possible antecedent , the number of pronoun-antecedent training examples fed to maxent is considerably higher than the numbers of pronouns shown in the table . )
perhaps one of the more striking facts is how well the algorithm performs with relatively few pronouns , which suggests that the generality of the features used allow for fairly reliable estimation without much data .
conclusion .
to conclude , a pronoun interpretation system can be trained solely on raw data using a standard set of morphosyntactic features to achieve performance that approaches that of a state-of-the-art supervised system .
although the self-acquired training data is no doubt highly noisy , the resulting model is still accurate enough to perform well at selecting correct antecedents .
as a next step , we will take a closer look at the training data acquired to try to ascertain 3tdt segment 14 , which is smaller than the others , provided only about 3800 pronouns in the runs corresponding to the last two rows of table 1 .
the overall average performance figures are the same to the first decimal place whether or not the results from this segment are included. the underlying reasons for this success .
there are also a number of variants of the algorithm that could be pursued .
for instance , whereas our algorithm uses the current models probabilities in a winner-take-all strategy for positive example selection , these probabilities could instead be used to dictate the likelihood that examples are assigned a positive outcome , or they could be thresholded in various ways to create a more discerning positive outcome assignment mechanism .
such strategies would avoid the current simplification of assigning a positive outcome to exactly one potential antecedent for each pronoun .
the relative generality of our feature set was appropriate given the size of the data sets used .
the availability of very large raw corpora , however , creates the prospect of using self-training with considerably more fine-grained features than is possible in a supervised scenario , due to the relative infrequency with which they would be found in any corpus of a size that could be feasibly annotated manually .
it is thus at least conceivable that a self-trained approach , coupled with a large set of features and a large corpus of raw data , could eventually overtake the performance of the best supervised models .
