applying cogex to recognize textual entailment .
abstract .
the pascal rte challenge has helped lcc to explore the applicability of enhancements that have been made to our logic form representation and wordnet lexical chains generator .
our system transforms each t-h pair into logic form representation with semantic relations .
the system automatically generates nlp axioms serving as linguistic rewriting rules and lexical chain axioms that connect concepts in the hypothesis and text .
a light set of simple hand-coded world knowledge axioms are also included .
our cogex logic prover is then used to attempt to prove entailment .
semantic relations , wordnet lexical chains , and nlp axioms all helped the logic prover detect entailment .
introduction .
textual entailment occurs when one text can be inferred from the meaning / contents of another text passage .
all assertions made in an entailed sentence must be made in the text passage directly , or be logically derivable from it .
our approach attempts to recognize textual entailment by determining if the hypothesis sentence can be logically derived from the text passage using a logic prover .
the goal of a logic prover is to determine if some hypothetical statement can be proven given a set of other known true statements .
our logic prover operates by reductio ad absurdum or proof by contradiction ( wos , 1988 ) .
the hypothesis is negated , and if it then contradicts anything in the text or anything inferred from the text , the prover concludes that the original hypothetical statement is derivable from the text ( thus , entailment exists ) .
a description of our systems implementation is provided in section 2 , our results and some performance analysis are included in section 3 , and some final concluding remarks are made in section 4 .
system description .
logic form transformation .
in the first stage of our system , the input text and hypothesis are converted into logic forms ( moldovan and rus , 2001 ) .
this conversion process includes part-of-speech tagging , parse tree generation , word sense disambiguation , and semantic relations detection .
in the final representation , word senses are removed from the predicates .
we found that the inaccuracy of the word sense disambiguator was so great that it prevented many of our other tools from being properly utilized .
axiom generation .
we have implemented our cogex ( moldovan et al. , 2003 ) logic prover into the entailment recognition system .
cogex is a modified version of the otter ( mccune , 1994 ) logic prover that has been adapted for natural language processing .
the prover requires a list of clauses called the set of support which is used to initiate the search for inferences .
the set of support is loaded with the negated form of the hypothesis as well as the predicates that make up the text passage .
a second list , called the usable list , contains clauses used by otter to generate inferences .
in our system the usable list consists of all the axioms that have been generated either automatically or by hand .
axioms in our system are utilized to provide external world knowledge , knowledge of syntactic equivalence between logic form predicates , and lexical knowledge in the form of lexical chains .
world knowledge axioms .
we incorporate a small common-sense knowledge base of 310 world knowledge axioms , where 80 have been manually designed based on the development set data , and 230 originate from previous projects .
currently , this data set is too small to have a significant impact , but in combination with lexical chains , the coverage of these axioms will grow .
nlp axioms .
our nlp axioms are linguistic rewriting rules that help break down complex logic structures and express syntactic equivalence .
these axioms are automatically generated by the system through logic form and parse tree analysis .
axioms are generated to break down complex nominals and coordinating conjunctions into their components so that other axioms can be applied to the components individually to generate a larger set of inferences .
other axioms help us : ( 1 ) establish equivalence between prepositions , ( 2 ) establish equivalence between different parts of speech , ( 3 ) equate words that have multiple noun forms , and ( 4 ) equate substantives within appositions .
wordnet lexical chains .
wordnet provides links between synsets .
each synset has a set of corresponding predicates for each word in the synonym set .
the name of a predicate is formed by synonym word form , its part of speech , and wordnet sense .
a predicate can have one or more arguments .
the predicates corresponding to noun synsets usually have a single argument and the predicates corresponding to verb synsets have three arguments : event , subject , and object arguments .
a lexical chain is a chain of relations between two synsets .
for each relation in the chain , the system generates an axiom using the predicates corresponding to the synsets in the relation .
the axiom states that the predicate from the first synset implies the predicate from the second .
for example , there is an entailment relation between the verbs buy and pay .
the system generates the following axiom for this relation : these axioms help the logic prover infer target concepts from starting concepts when lexical chains are found between the two .
not all wordnet relations are used for generating axioms .
the following three classes of relations are used : pure wordnet relations , relations created from wordnet derivational morphology , and relations extracted from word- net glosses .
a detailed description of the system as a whole can be found in ( novischi , 2005 ) and ( moldovan and novischi , 2002 ) .
logic prover .
once the set of support and usable lists are complete , the logic prover can begin searching for proofs .
the clauses in the set of support list are weighted in the order in which they should be chosen to participate in the search .
the negated hypothesis is assigned the largest weight to ensure that it will be the last clause to participate in the search .
the logic prover removes the clause with the smallest weight from the set of support , and searches the usable list for new inferences that can be made .
any inferences that are produced are assigned an appropriate weight depending on what axiom they were derived from and appended to the set of support list .
the logic prover continues in this fashion until the set of support list is empty .
if a refutation is found , then the proof is complete .
if a refutation cannot be found , then predicate arguments are relaxed .
if argument relaxation fails to produce a refutation , predicates are dropped from the negated hypothesis until a refutation is found .
once a proof by refutation is found , a score for that proof is calculated by starting with an initial perfect score and deducting points for axioms that are utilized in the proof , arguments that are relaxed , and predicates that are dropped .
scoring .
the score generated by the logic prover is only a measure of the kinds of axioms used in the proof and the significance of the dropped arguments and predicates .
t-h pairs with longer sentences can potentially drop more predicates , resulting in lower prover scores .
scores are normalized by first calculating the maximum penalty that can be assessed to a pair by dropping all of the hypothesis predicates .
the penalty assessed by the logic prover is then divided by the maximum drop penalty to determine the normalized score .
due to the logic provers relaxation techniques , it is always successful in producing a proof .
the determination of whether entailment exists is made by examining the penalties assessed by the logic prover in the process of generating the proof .
as more axioms are utilized and more predicates are dropped , it becomes much less likely that entailment exists between a pair .
all normalized prover scores that fall below a specified threshold are considered false entailment and all scores that are above the threshold are considered true entailment .
an appropriate threshold is calculated by examining the scoring output of the development data set to determine what threshold produces the highest accuracy .
the confidence score for a t-h pair in our system is measured as the distance between the normalized score and the threshold .
normalized scores that are further from the threshold will have a higher confidence score than normalized scores that are closer to the threshold .
the difference between the normalized score and the threshold itself is normalized such that the resulting confidence score is a value between zero and one .
performance evaluation .
our results for the challenge are summarized in table 1 .
as evidenced by these results , our system performs significantly better on t-h pairs in the comparable documents task .
due to the way t-h pairs are chosen in this task , there is often little to no information in the text of false pairs that could help us logically infer the hypothesis .
this inferencing inability causes the logic prover to drop a large number of predicates and return extremely low scores for the false entailment pairs .
the large difference between the true and false entailment scores allows us to easily separate the pairs .
the average scores for true and false entailment varied significantly over all of the tasks .
this large variance makes it extremely difficult to choose a single threshold that can be used to detect entailment for all of the tasks .
by selecting thresholds specific to each task , we were able to increase the test sets accuracy to .562 .
this accuracy is still considerably lower than the accuracy we received on the development set from which the thresholds were chosen .
the numerous disagreements we had amongst ourselves and with the gold standard annotations leads us to believe to that the only appropriate way to calculate an upper bound for this task is to utilize the kappa agreement metric .
however , without a large set of different human annotations for the data set , it is impossible to calculate this metric .
before evaluating the t-h pairs in the test set with our system , we manually determined how difficult it is to prove entailment in each of the true entailment t-h pairs .
we established five different difficulty levels : easy , moderate , difficult , intractable , and invalid .
proofs are considered easy in cases where the entailment is simply a matter of eliminating information from the first sentence , recognizing an apposition or replacing one or two words with synonyms .
consider the following example : the expectation is that all entailment pairs that have been deemed easy or moderate can be handled by our current system implementation .
difficult proofs are those that cannot be handled by our theorem prover without adding substantial new functionality ( coreference resolution , predicate variables in rules , etc . ) or without using ad hoc rules ( those not applicable beyond the case which motivates them ) .
the following example requires very specific axioms and coreference resolution : text : israeli prime minister ariel sharon threatened to dismiss cabinet ministers who dont support his plan to withdraw from the gaza strip .
hypothesis : israeli prime minister ariel sharon threatened to fire cabinet opponents of his gaza withdrawal plan .
we have labeled t-h pairs as intractable if we believe that entailment could not be correctly detected by an automated system .
invalid is used to indicate that , in our opinion , an entailment pair which was labeled true should have been labeled false .
in the following pair the text does not imply that silvio berlusconi is prime minister of italy , only that he is a prime minister with a mandate to reform italy .
the systems performance on the t-h pairs classified as easy or moderate is significantly better than its performance on other pairs as illustrated in table 2 .
since many of the t-h pairs with the moderate classification require some external world knowledge , we suspect that with a larger knowledge base , the accuracy of the t-h pairs classified as moderate would be significantly higher .
it may be possible to build a classifier to determine the inference difficulty and only return results for pairs it deems to be easy or moderate .
the main difficulty with such an approach is that it is hard to classify the difficulty of an inference without knowing whether the inference is true or false .
we suspect that a difficulty classifier would have trouble distinguishing difficult true entailments from easy false entailments , and vice versa .
conclusion .
we participated in the rte challenge mainly as a learning experience and a test of our existing logic prover system implemented in a new way .
adding semantic relations to the logic form provided deeper semantic connectivity between concepts .
this made it possible to write more abstract ( more generally- applicable ) world knowledge axioms .
wordnet lexical chains helped to connect related concepts that used different words or different forms of the same word .
and finally , based on linguistic patterns , the nlp axioms helped to link concepts that would otherwise not be connected in the logic form transformation .
