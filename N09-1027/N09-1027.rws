There have been significant efforts in the both the
monolingual parsing and machine translation literature
to address the impact of the MAP approximation
and the choice of labels in their respective
models; we survey the work most closely related to
our approach. (May and Knight 2006) extract nbest
lists containing unique translations rather than
unique derivations, while (Kumar and Byrne 2004)
use the Minimum Bayes Risk decision rule to select
the lowest risk (highest BLEU score) translation
rather than derivation from an n-best list. (Tromble et al. 2008) extend this work to lattice structures.
All of these approaches only marginalize over alternative
candidate derivations generated by a MAPdriven
decoding process. More recently, work by
(Blunsom et al. 2007) propose a purely discriminative
model whose decoding step approximates the
selection of the most likely translation via beam
search. (Matsusaki et al. 2005) and (Petrov et al. 2006) propose automatically learning annotations
that add information to categories to improve monolingual
parsing quality. Since the parsing task requires
selecting the most non-annotated tree, the annotations
add an additional level of structure that
must be marginalized during search. They demonstrate
improvements in parse quality only when a
variational approximation is used to select the most
likely unannotated tree rather than simply stripping
annotations from the MAP annotated tree. In our
work, we focused on approximating the selection of
the most likely unlabeled derivation during search,
rather than as a post-processing operation; the methods
described above might improve this approximation,
at some computational expense.
