lattice minimum bayes-risk decoding for statistical machine translation .
abstract .
we present minimum bayes-risk ( mbr ) decoding over translation lattices that compactly encode a huge number of translation hypotheses .
we describe conditions on the loss function that will enable efficient implementation of mbr decoders on lattices .
we introduce an approximation to the bleu score ( papineni et al. , 2001 ) that satisfies these conditions .
the mbr decoding under this approximate bleu is realized using weighted finite state automata .
our experiments show that the lattice mbr decoder yields moderate , consistent gains in translation performance over n-best mbr decoding on arabicto-english , chinese-to-english and englishto-chinese translation tasks .
we conduct a range of experiments to understand why lattice mbr improves upon n-best mbr and study the impact of various parameters on mbr performance .
introduction .
statistical language processing systems for speech recognition , machine translation or parsing typically employ the maximum a posteriori ( map ) decision rule which optimizes the 0-1 loss function .
in contrast , these systems are evaluated using metrics based on string-edit distance ( word error rate ) , n- gram overlap ( bleu score ( papineni et al. , 2001 ) ) , or precision / recall relative to human annotations .
minimum bayes-risk ( mbr ) decoding ( bickel and doksum , 1977 ) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification .
thus it directly incorporates the loss function into the decision criterion .
the approach has been shown to give improvements over the map classifier in many areas of natural language processing including automatic speech recognition ( goel and byrne , 2000 ) , machine translation ( kumar and byrne , 2004 ; zhang and gildea , 2008 ) , bilingual word alignment ( kumar and byrne , 2002 ) , and parsing ( goodman , 1996 ; titov and henderson , 2006 ; smith and smith , 2007 ) .
in statistical machine translation , mbr decoding is generally implemented by re-ranking an n-best list of translations produced by a first-pass decoder ; this list typically contains between 100 and 10,000 hypotheses .
kumar and byrne ( 2004 ) show that mbr decoding gives optimal performance when the loss function is matched to the evaluation criterion ; in particular , mbr under the sentence-level bleu loss function ( papineni et al. , 2001 ) gives gains on bleu .
this is despite the fact that the sentence-level bleu loss function is an approximation to the exact corpus-level bleu .
a different mbr inspired decoding approach is pursued in zhang and gildea ( 2008 ) for machine translation using synchronous context free grammars .
a forest generated by an initial decoding pass is rescored using dynamic programming to maximize the expected count of synchronous constituents in the tree that corresponds to the translation .
since each constituent adds a new 4-gram to the existing translation , this approach approximately maximizes the expected bleu .
in this paper we explore a different strategy to perform mbr decoding over translation lattices ( ueffing et al. , 2002 ) that compactly encode a huge number of translation alternatives relative to an n-best list .
this is a model-independent approach in that the lattices could be produced by any statistical mt system both phrase-based and syntax- based systems would work in this framework .
we will introduce conditions on the loss functions that can be incorporated in lattice mbr decoding .
we describe an approximation to the bleu score ( papineni et al. , 2001 ) that will satisfy these conditions .
our lattice mbr decoding is realized using weighted finite state automata .
we expect lattice mbr decoding to improve upon n-best mbr primarily because lattices contain many more candidate translations than the n- best list .
this has been demonstrated in speech recognition ( goel and byrne , 2000 ) .
we conduct a range of translation experiments to analyze lattice mbr and compare it with n-best mbr .
an important aspect of our lattice mbr is the linear approximation to the bleu score .
we will show that mbr decoding under this score achieves a performance that is at least as good as the performance obtained under sentence-level bleu score .
the rest of the paper is organized as follows .
we review mbr decoding in section 2 and give the formulation in terms of a gain function .
in section 3 , we describe the conditions on the gain function for efficient decoding over a lattice .
the implementation of lattice mbr with weighted finite state automata is presented in section 4 .
in section 5 , we introduce the corpus bleu approximation that makes it possible to perform efficient lattice mbr decoding .
an example of lattice mbr with a toy lattice is presented in section 6 .
we present lattice mbr experiments in section 7 .
a final discussion is presented in section 8 .
minimum bayes risk decoding .
minimum bayes-risk ( mbr ) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model ( bickel and doksum , 1977 ) .
we begin with a review of mbr decoding for statistical machine translation ( smt ) .
statistical mt ( brown et al. , 1990 ; och and ney , 2004 ) can be described as a mapping of a word sequence f in the source language to a word sequence e in the target language ; this mapping is produced by the mt decoder s ( f ) .
if the reference translation e is known , the decoder performance can be measured by the loss function l ( e , s ( f ) ) .
given such a loss function l ( e , e ' ) between an automatic translation e ' and the reference e , and an underlying probability model p ( eif ) , the mbr decoder has the following form ( goel and byrne , 2000 ; kumar and byrne , 2004 ) : we are interested in performing mbr decoding under a sentence-level bleu score ( papineni et al. , 2001 ) which behaves like a gain function : it varies between 0 and 1 , and a larger value reflects a higher similarity .
we will therefore use equation 1 as the mbr decoder .
we note that represents the space of translations .
for n-best mbr , this space is the n-best list produced by a baseline decoder .
we will investigate the use of a translation lattice for mbr decoding ; in this case , will represent the set of candidates encoded in the lattice .
in general , mbr decoding can use different spaces for hypothesis selection and risk computation : argmax and the sum in equation 1 ( goel , 2001 ) .
as an example , the hypothesis could be selected from the n-best list while the risk is computed based on the entire lattice .
therefore , the mbr decoder can be more generally written as follows : lattice mbr decoding .
we now present mbr decoding on translation lattices .
a translation word lattice is a compact representation for very large n-best lists of translation hypotheses and their likelihoods .
formally , it is an acyclic weighted finite state acceptor ( wfsa ) ( mohri , 2002 ) consisting of states and arcs representing transitions between states .
each arc is labeled with a word and a weight .
each path in the lattice , consisting of consecutive transitions beginning at the distinguished initial state and ending at a final state , expresses a candidate translation .
aggregation of the weights along the path1 produces the weight of the paths candidate h ( e , f ) according to the model .
in our setting , this weight will imply the posterior probability of the translation e given the source sentence f : because a lattice may represent a number of candidates exponential in the size of its state set , it is often impractical to compute the mbr decoder ( equation 1 ) directly .
however , if we can express the gain function g as a sum of local gain functions gi , then we now show that equation 1 can be refactored and the mbr decoder can be computed efficiently .
we loosely call a gain function local if it can be applied to all paths in the lattice via wfsa intersection ( mohri , 2002 ) without significantly multiplying the number of states .
wfsa mbr computations .
we now show how the lattice mbr decision rule ( equation 6 ) can be implemented using weighted finite state automata ( mohri , 1997 ) .
there are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system .
we will describe these steps in the setting where the evidence lattice e may be different from the hypothesis lattice h ( equation 2 ) .
extract the set of n-grams that occur in the evidence lattice e .
for the usual bleu score , n ranges from one to four .
compute the posterior probability p ( wl ) of each of these n-grams .
intersect each n-gram w , with an appropriate weight ( from equation 6 ) , to an initially unweighted copy of the hypothesis lattice h .
find the best path in the resulting automaton .
computing the set of n-grams n that occur in a finite automaton requires a traversal , in topological order , of all the arcs in the automaton .
because the lattice is acyclic , this is possible .
each state q in the automaton has a corresponding set of n-grams nq ending there .
for each state q , nq is initialized to { e } , the set containing the empty n-gram .
each arc in the automaton extends each of its source states n-grams by its word label , and adds the resulting n-grams to the set of its target state . ( e arcs do not extend n-grams , but transfer them unchanged . ) n-grams longer than the desired order are discarded .
n is the union over all states q of nq .
we successively intersect each of these automata with an automaton that begins as an unweighted copy of the lattice h .
this automaton must also incorporate the factor 00 of each word .
this can be accomplished by intersecting the unweighted lattice with the automaton accepting ( e / 00 ) * .
the resulting mbr automaton computes the total expected gain of each path .
a path in this automaton that corresponds to the word sequence e ' has cost : 00le 'l + e , ,,car0 , , , # , ,, ( e ) p ( wl ) ( expression within the curly brackets in equation 6 ) .
finally , we extract the best path from the resulting automaton4 , giving the lattice mbr candidate translation according to the gain function ( equation 6 ) .
linear corpus bleu .
our lattice mbr formulation relies on the decomposition of the overall gain function as a sum of local gain functions ( equation 5 ) .
we here describe a linear approximation to the log ( bleu score ) ( papineni et al. , 2001 ) which allows such a decomposition .
this will enable us to rewrite the log ( bleu ) as a linear function of n-gram matches and the hypothesis length .
our strategy will be to use a first order taylor-series approximation to what we call the corpus log ( bleu ) gain : the change in corpus log ( bleu ) contributed by the sentence relative to not including that sentence in the corpus .
the corpus log bleu gain is approximated by a first-order vector taylor series expansion about the initial values of cn .
substituting the derivatives in equation 8 gives where each acn = c 'n cn counts the statistic in the sentence of interest , rather than the corpus as a whole .
this score is therefore a linear function in counts of words ac0 and n-gram matches acn .
our approach ignores the count clipping present in the exact bleu score where a correct n-gram present once in the reference but several times in the hypothesis will be counted only once as correct .
such an approach is also followed in dreyer et al. ( 2007 ) .
we now describe how the n-gram factors ( equation 11 ) are computed .
the factors depend on a set of n-gram matches and counts ( cn ; n e 10 , 1 , 2 , 3 , 4 } ) .
these factors could be obtained from a decoding run on a development set .
however , doing so could make the performance of lattice mbr very sensitive to the actual bleu scores on a particular run .
we would like to avoid such a dependence and instead , obtain a set of parameters which can be estimated from multiple decoding runs without mbr .
to achieve this , we make use of the properties of n-gram matches .
it is known that the average n- gram precisions decay approximately exponentially with n ( papineni et al. , 2001 ) .
we now assume that the number of matches of each n-gram is a constant ratio r times the matches of the corresponding n 1 gram .
if the unigram precision is p , we can obtain the n-gram factors ( n e 11 , 2 , 3 , 4 } ) ( equation 11 ) as a function of the parameters p and r , and the number of unigram tokens t : we set p and r to the average values of unigram precision and precision ratio across multiple development sets .
substituting the above factors in equation 6 , we find that the mbr decision does not depend on t ; therefore any value of t can be used .
an example .
figure 1 shows a toy lattice and the final mbr automaton ( section 4 ) for bleu with a maximum n- gram order of 2 .
we note that the mbr hypothesis ( bcde ) has a higher decoder cost relative to the map hypothesis ( abde ) .
however , bcde gets a higher expected gain ( equation 6 ) than abde since it shares more n-grams with the rank-3 hypothesis ( bcda ) .
this illustrates how a lattice can help select mbr translations that can differ from the map translation .
experiments .
we now present experiments to evaluate mbr decoding on lattices under the linear corpus bleu path in the bottom is the mbr hypothesis .
the precision parameters in equation 12 are set to : development and blind test sets .
we present our experiments on the constrained data track of the nist 2008 arabic-to-english ( aren ) , chinese-to-english ( zhen ) , and english-to-chinese ( enzh ) machine translation tasks.5 in all language pairs , the parallel and monolingual data consists of all the allowed training sets in the constrained track .
for each language pair , we use two development sets : one for minimum error rate training ( och , 2003 ; macherey et al. , 2008 ) , and the other for tuning the scale factor for mbr decoding .
our development sets consists of the nist 2004 / 2003 evaluation sets for both aren and zhen , and nist 2006 ( nist portion ) / 2003 evaluation sets for enzh .
we report results on nist 2008 which is our blind test set .
statistics computed over these data sets are reported in table 1 .
our phrase-based statistical mt system is similar to the alignment template system described in och and ney ( 2004 ) .
the system is trained on parallel corpora allowed in the constrained track .
we first perform sentence and sub-sentence chunk alignment on the parallel documents .
we then train word alignment models ( och and ney , 2003 ) using 6 model-1 iterations and 6 hmm iterations .
an additional 2 iterations of model-4 are performed for zhen and enzh pairs .
word alignments in both source-to-target and target-to-source directions are obtained using the maximum a-posteriori ( map ) framework ( matusov et al. , 2004 ) .
an inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments .
several feature functions are then computed over the phrase- pairs . 5-gram word language models are trained on the allowed monolingual corpora .
minimum error rate training under bleu is used for estimating approximately 20 feature function weights over the dev1 development set .
translation is performed using a standard dynamic programming beam-search decoder ( och and ney , 2004 ) using two decoding passes .
the first decoder pass generates either a lattice or an n-best list .
mbr decoding is performed in the second pass .
the mbr scaling parameter ( a in equation 3 ) is tuned on the dev2 development set .
translation results .
we next report translation results from lattice mbr decoding .
all results will be presented on the nist 2008 evaluation sets .
we report results using the nist implementation of the bleu score which computes the brevity penalty using the shortest reference translation for each segment ( nist , 2002 2008 ) .
the bleu scores are reported at the word- level for aren and zhen but at the character level for enzh .
we measure statistical significance using 95 % confidence intervals computed with paired bootstrap resampling ( koehn , 2004 ) .
in all tables , systems in a column show statistically significant differences unless marked with an asterisk .
we first compare lattice mbr to n-best mbr decoding and map decoding ( table 2 ) .
in these experiments , we hold the likelihood scaling factor a constant ; it is set to 0.2 for aren and enzh , and 0.1 for zhen .
the translation lattices are pruned using forward-backward pruning ( sixtus and ortmanns , 1999 ) so that the average numbers of arcs per word ( lattice density ) is 30 .
for n-best mbr , we use n-best lists of size 1000 .
to match the loss function , lattice mbr is performed at the word level for aren / zhen and at the character level for enzh .
our lattice mbr is implemented using the google openfst library.6 in our experiments , p , r ( equation 12 ) have values of 0.85 / 0.72 , 0.80 / 0.62 , and 0.63 / 0.48 for aren , zhen , and enzh respectively .
we note that lattice mbr provides gains of 0.2- 1.0 bleu points over n-best mbr , which in turn gives 0.2-0.6 bleu points over map .
these gains are obtained on top of a baseline system that has competitive performance relative to the results reported in the nist 2008 evaluation.7 this demonstrates the effectiveness of lattice mbr decoding as a realization of mbr decoding which yields substantial gains over the n-best implementation .
the gains from lattice mbr over n-best mbr could be due to a combination of factors .
these include : 1 ) better approximation of the corpus bleu score , 2 ) larger hypothesis space , and 3 ) larger evidence space .
we now present experiments to tease apart these factors .
our first experiment restricts both the hypothesis and evidence spaces in lattice mbr to the 1000-best list ( table 3 ) .
we compare this to n-best mbr with : a ) sentence-level bleu , and b ) sentence-level log bleu .
the results show that when restricted to the 1000- best list , lattice mbr performs slightly better than n-best mbr ( with sentence bleu ) on aren / enzh while n-best mbr is better on zhen .
we hypothesize that on aren / enzh , the linear corpus bleu gain ( equation 10 ) is better correlated to the actual corpus bleu than sentence-level bleu while the opposite is true on zhen .
n-best mbr gives similar results with either sentence bleu or sentence log bleu .
this confirms that using a log bleu score does not change the outcome of mbr decoding and further justifies our taylor-series approximation of the log bleu score .
we next attempt to understand factors 2 and 3 .
to do that , we carry out lattice mbr when either the hypothesis or the evidence space in equation 2 is restricted to 1000-best hypotheses ( table 4 ) .
for comparison , we also include results from lattice mbr when both hypothesis and evidence spaces are identical : either the full lattice or the 1000-best list ( from tables 2 and 3 ) .
these results show that lattice mbr results are almost unchanged when the hypothesis space is restricted to a 1000-best list .
however , when the evidence space is shrunk to a 1000-best list , there is a significant degradation in performance ; these latter results are almost identical to the scenario when both evidence and hypothesis spaces are restricted to the 1000-best list .
this experiment throws light on what makes lattice mbr effective over n-best mbr .
relative to the n-best list , the translation lattice provides a better estimate of the expected bleu score .
on the other hand , there are few hypotheses outside the 1000-best list which are selected by lattice mbr .
finally , we show how the performance of lattice mbr changes as a function of the lattice density .
the lattice density is the average number of arcs per word and can be varied using forward-backward pruning ( sixtus and ortmanns , 1999 ) .
figure 2 reports the average number of lattice paths and bleu scores as a function of lattice density .
the results show that lattice mbr performance generally improves when the size of the lattice is increased .
however , on zhen , there is a small drop beyond a density of 10 .
this could be due to low quality ( low posterior probability ) hypotheses that get included at the larger densities and result in a poorer estimate of the expected bleu score .
on aren and enzh , there are some gains beyond a lattice density of 30 .
these gains are relatively small and come at the expense of higher memory usage ; we therefore work with a lattice density of 30 in all our experiments .
we note that lattice mbr is operating over lattices which are gigantic in comparison to the number of paths in an n-best list .
at a lattice density of 30 , the lattices in aren contain on an average about 1081 hypotheses ! 7.4 lattice mbr scale factor we next examine the role of the scale factor a in lattice mbr decoding .
the mbr scale factor determines the flatness of the posterior distribution ( equation 3 ) .
it is chosen using a grid search on the dev2 set ( table 1 ) .
figure 3 shows the variation in bleu scores on eval08 as this parameter is varied .
the results show that it is important to tune this factor .
the optimal scale factor is identical for all three language pairs .
in experiments not reported in this paper , we have found that the optimal scaling factor on a moderately sized development set carries over to unseen test sets .
maximum n-gram order .
lattice mbr decoding ( equation 6 ) involves computing a posterior probability for each n-gram in the lattice .
we would like to speed up the lattice mbr computation ( section 4 ) by restricting the maximum order of the n-grams in the procedure .
the results ( table 5 ) show that on aren , there is no degradation if we limit the maximum order of the n-grams to 3 .
however , on zhen / enzh , there is improvement by considering 4-grams .
we can therefore reduce lattice mbr computations in aren .
discussion .
we have presented a procedure for performing minimum bayes-risk decoding on translation lattices .
this is a significant development in that the mbr decoder operates over a very large number of translations .
in contrast , the current n-best implementation of mbr can be scaled to , at most , a few thousands of hypotheses .
if the number of hypotheses is greater than , say 20,000 , the n-best mbr becomes computationally expensive .
the lattice mbr technique is efficient when performed over enormous number of hypotheses ( up to 1080 ) since it takes advantage of the compact structure of the lattice .
lattice mbr gives consistent improvements in translation performance over n-best mbr decoding , which is used in many state-of-the-art research translation systems .
moreover , we see gains on three different language pairs .
there are two potential reasons why lattice mbr decoding could outperform n-best mbr : a larger hypothesis space from which translations could be selected or a larger evidence space for computing the expected loss .
our experiments show that the main improvement comes from the larger evidence space : a larger set of translations in the lattice provides a better estimate of the expected bleu score .
in other words , the lattice provides a better posterior distribution over translation hypotheses relative to an n- best list .
this is a novel insight into the workings of mbr decoding .
we believe this could be possibly employed when designing discriminative training approaches for machine translation .
more generally , we have found a component in machine translation where the posterior distribution over hypotheses plays a crucial role .
we have shown the effect of the mbr scaling factor on the performance of lattice mbr .
the scale factor determines the flatness of the posterior distribution over translation hypotheses .
a scale of 0.0 means a uniform distribution while 1.0 implies that there is no scaling .
this is an important parameter that needs to be tuned on a development set .
there has been prior work in mbr speech recognition and machine translation ( goel and byrne , 2000 ; ehling et al. , 2007 ) which has shown the need for tuning this factor .
our mt system parameters are trained with minimum error rate training which assigns a very high posterior probability to the map translation .
as a result , it is necessary to flatten the probability distribution so that mbr decoding can select hypotheses other than the map hypothesis .
our lattice mbr implementation is made possible due to the linear approximation of the bleu score .
this linearization technique has been applied elsewhere when working with bleu : smith and eisner ( 2006 ) approximate the expectation of log bleu score .
in both cases , a linear metric makes it easier to compute the expectation .
while we have applied lattice mbr decoding to the approximate bleu score , we note that our procedure ( section 3 ) is applicable to other gain functions which can be decomposed as a sum of local gain functions .
in particular , our framework might be useful with transla tion metrics such as ter ( snover et al. , 2006 ) or meteor ( lavie and agarwal , 2007 ) .
in contrast to a phrase-based smt system , a syntax based smt system ( e.g. zollmann and venugopal ( 2006 ) ) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures .
we believe that our lattice mbr framework can be extended to such hypergraphs with loss functions that take into account both bleu scores as well as parse tree structures .
lattice and forest based search and training procedures are not yet common in statistical machine translation .
however , they are promising because the search space of translations is much larger than the typical n-best list ( mi et al. , 2008 ) .
we hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear , global loss functions such as bleu .
