extracting structural paraphrases from aligned monolingual corpora .
abstract .
we present an approach for automatically learning paraphrases from aligned monolingual corpora .
our algorithm works by generalizing the syntactic paths between corresponding anchors in aligned sentence pairs .
compared to previous work , structural paraphrases generated by our algorithm tend to be much longer on average , and are capable of capturing long-distance dependencies .
in addition to a standalone evaluation of our paraphrases , we also describe a question answering application currently under development that could immensely benefit from automatically-learned structural paraphrases .
introduction .
the richness of human language allows people to express the same idea in many different ways ; they may use different words to refer to the same entity or employ different phrases to describe the same concept .
acquisition of paraphrases , or alternative ways to convey the same information , is critical to many natural language applications .
for example , an effective question answering system must be equipped to handle these variations , because it should be able to respond to differently phrased natural language questions .
while there are many resources that help systems deal with single-word synonyms , e.g. , word- net , there are few resources for multiple-word or domain-specific paraphrases .
because manually collecting paraphrases is time-consuming and impractical for large-scale applications , attention has recently focused on techniques for automatically acquiring paraphrases .
we present an unsupervised method for acquiring structural paraphrases , or fragments of syntactic trees that are roughly semantically equivalent , from aligned monolingual corpora .
the structural paraphrases produced by our algorithm are similar to the s-rules advocated by katz and levin for question answering ( 1988 ) , except that our paraphrases are automatically generated .
because there is disagreement regarding the exact definition of paraphrases ( dras , 1999 ) , we employ that operating definition that structural paraphrases are roughly interchangeable within the specific configuration of syntactic structures that they specify .
our approach is a synthesis of techniques developed by barzilay and mckeown ( 2001 ) and lin and pantel ( 2001 ) , designed to overcome the limitations of both .
in addition to the evaluation of paraphrases generated by our method , we also describe a novel information retrieval system under development that is designed to take advantage of structural paraphrases .
previous work .
there has been a rich body of research on automatically deriving paraphrases , including equating morphological and syntactic variants of technical terms ( jacquemin et al. , 1997 ) , and identifying equivalent adjective-noun phrases ( lapata , 2001 ) .
unfortunately , both are limited in types of paraphrases that they can extract .
other researchers have explored distributional clustering of similar words ( pereira et al. , 1993 ; lin , 1998 ) , but it is unclear to what extent such techniques produce paraphrases .
most relevant to this paper is the work of barzilay and mckeown and the work of lin and pantel .
barzilay and mckeown ( 2001 ) extracted both single- and multiple-word paraphrases from a sentence-aligned corpus for use in multi-document summarization .
they constructed an aligned corpus from multiple translations of foreign novels .
from this , they co-trained a classifier that decided whether or not two phrases were paraphrases of each other based on their surrounding context .
barzilay and mckeown collected 9483 paraphrases with an average precision of 85.5 % .
however , 70.8 % of the paraphrases were single words .
in addition , the paraphrases were required to be contiguous .
lin and pantel ( 2001 ) used a general text corpus to extract what they called inference rules , which we can take to be paraphrases .
in their algorithm , rules are represented as dependency tree paths between two words .
the words at the ends of a path are considered to be features of that path .
for each path , they recorded the different features ( words ) that were associated with the path and their respective frequencies .
lin and pantel calculated the similarity of two paths by looking at the similarity of their features .
this method allowed them to extract inference rules of moderate length from general corpora .
however , the technique is computationally expensive , and furthermore can give misleading results , i.e. , paths having the opposite meaning often share similar features .
approach .
our approach , like barzilay and mckeowns , is built on the application of sentence-alignment techniques used in machine translation to generate paraphrases .
the insight is simple : if we have pairs of sentences with the same semantic content , then the difference in lexical content can be attributed to variations in the surface form .
by generalizing these differences we can automatically derive paraphrases .
barzilay and mckeown perform this learning process by only considering the local context of words and their frequencies ; as a result , paraphrases must be contiguous , and in the majority of cases , are only one word long .
we believe that disregarding the rich syntactic structure of language is an oversimplification , and that structural paraphrases offer several distinct advantages over lexical paraphrases .
long distance relations can be captured by syntactic trees , so that words in the paraphrases do not need to be contiguous .
use of syntactic trees also buffers against morphological variants ( e.g. , different inflections ) and some syntactic variants ( e.g. , active vs. passive ) .
finally , because paraphrases are context-dependent , we believe that syntactic structures can encapsulate a richer context than lexical phrases .
based on aligned monolingual corpora , our technique for extracting paraphrases builds on lin and pantels insight of using dependency paths ( derived from parsing ) as the fundamental unit of learning and using parts of those paths as features .
based on the hypothesis that paths between identical words in aligned sentences are semantically equivalent , we can extract paraphrases by scoring the path frequency and context .
our approach addresses the limitations of both barzilay and mckeowns and lin and pantels work : using syntactic structures allows us to generate structural paraphrases , and using aligned corpora renders the process more computationally tractable .
the following sections describe our approach in greater detail .
corpus alignment .
multiple english translations of foreign novels , e.g. , twenty thousand leagues under the sea by jules verne , were used for extraction of paraphrases .
although translations by different authors differ slightly in their literary interpretation of the original text , it was usually possible to find corresponding sentences that have the same semantic content .
sentence alignment was performed using the gale and church algorithm ( 1991 ) with the following cost function : to test the accuracy of our alignment , we manually aligned 454 sentences from two different versions of chapter 21 from twenty thousand leagues under the sea and compared the results of our automatic alignment algorithm against the manually generated gold standard .
we obtained a precision of 0.93 and recall of 0.88 , which is comparable to the numbers ( p.94 / r.85 ) reported by barzilay and mckeown , who used a different cost function for the alignment process .
parsing and postprocessing .
the sentence pairs produced by the alignment algorithm are then parsed by the link parser ( sleator and temperly , 1993 ) , a dependency-based parser developed at cmu .
the resulting parse structures are post-processed to render the links more consistent : because the link parser does not directly identify the subject of a passive sentence , our postprocessor takes the object of the by-phrase as the subject by default .
for our purposes , auxiliary verbs are ignored ; the postprocessor connects verbs directly to their subjects , discarding links through any auxiliary verbs .
in addition , subjects and objects within relative clauses are appropriately modified so that the linkages remained consistent with subject and object linkages in the matrix clause .
for sentences involving verbs that have particles , the link parser connects the object of the verb directly to the verb itself , attaching the particle separately .
our postprocessor modifies the link structure so that the object is connected to the particle in order to form a continuous path .
predicate adjectives are converted into an adjective-noun modification link instead of a complete verb-argument structure .
also , common nouns denoting places and people are marked by consulting wordnet .
paraphrase extraction .
the paraphrase extraction process starts by finding anchors within the aligned sentence pairs .
in our approach , only nouns and pronouns serve as possible anchors .
the anchor words from the sentence pairs are brought into alignment and scored by a simple set of ordered heuristics : exact string matches denote correspondence .
noun and matching pronoun ( same gender and number ) denote correspondence .
such a match penalizes the score by 50 % .
unique semantic class ( e.g. , places and people ) denotes correspondence .
such a match penalizes the score by 50 % .
unique part of speech ( i.e. , the only noun pair in the sentences ) denotes correspondence .
such a match penalizes the score by 50 % .
otherwise , attempt to find correspondence by finding longest common substrings .
such a match penalizes the score by 50 % .
if a word occurs more than once in the aligned sentence pairs , all possible combinations are considered , but the score for such a corresponding anchor pair is further penalized by 50 % .
for each pair of anchors , a breadth-first search is used to find the shortest path between the anchor words .
the search algorithm explicitly rejects paths that contain conjunctions and punctuation .
if valid paths are found between anchor pairs in both of the aligned sentences , the resulting paths are considered candidate paraphrases , with a default score of one ( subjected to penalties imposed by imperfect anchor matching ) .
scores of candidate paraphrases take into account two factors : the frequency of anchors with respect to a particular candidate paraphrase and the variety of different anchors from which the paraphrase was produced .
the initial default score of any paraphrase is one ( assuming perfect anchor matches ) , but for each additional occurrence the score is incremented by 2 n , where n is the number of times the current set of anchors has been seen .
therefore , the effect of seeing new sets of anchors has a big initial impact on the score , but the additional increase in score is subjected to diminishing returns as more occurrences of the same anchor are encountered .
using the approach described in previous sections , we were able to extract nearly six thousand different paraphrases ( see table 1 ) from our corpus , which consisted of two translations of 20,000 leagues under the sea , two translations of the kreutzer sonata , and three translations of madame bouvary .
our corpus was essentially the same as the one used by barzilay and mckeown , with the exception of some short fairy tale translations that we found to be unsuitable .
due to the length of sentences ( some translations were noted for their paragraph-length sentences ) , the link parser was unable to produce a parse for approximately eight percent of the sentences .
although the link parser is capable of producing partial linkages , accuracy deteriorated significantly as the length of the input string increased .
the distribution of paraphrase length is shown in figure 1 .
the length of paraphrases is measured by the number of words that it contains ( discounting the anchors on both sides ) .
to evaluate the accuracy of our results , 130 paraphrases were roughly interchangeable with each other , given the context of the genre .
we believe that the genre constraint was important because some paraphrases captured literary or archaic uses of particular words that were not generally useful .
this should not be viewed as a shortcoming of our approach , but rather an artifact of our corpus .
in addition , sample sentences containing the structural paraphrases were presented as context to the judges ; structural paraphrases are difficult to comprehend without this information .
a summary of the judgments provided by human evaluators is shown in table 2 .
the average precision of our approach stands at just over forty percent ; the average length of the paraphrases learned was 3.26 words long .
our results also show that judging structural paraphrases is a difficult task and inter-assessor agreement is rather low .
all of the evaluators agreed on the judgments ( either positive or negative ) only 75.4 % of the time .
the average correlation constant of the judgments is only 0.66 .
the highest scoring paraphrase was the equivalence of the possessive morpheme s with the preposition of .
we found it encouraging that our algorithm was able to induce this structural paraphrase , complete with co-indexed anchors on the ends of the paths , i.e. , as b ~ b of a. some other interesting examples include : a more detailed breakdown of the evaluation results can be seen in table 3 .
increasing the threshold for generating paraphrases tends to increase their precision , up to a certain point .
in general , the highest ranking structural paraphrases consisted of single word paraphrases of prepositions , e.g. , at in .
our algorithm noticed that different prepositions were often interchangeable , which is something that our human assessors disagreed widely on .
beyond a certain threshold , the accuracy of our approach actually decreases .
discussion .
an obvious first observation about our algorithm is the dependence on parse quality ; bad parses lead to many bogus paraphrases .
although the parse results from the link parser are far from perfect , it is unclear whether other purely statistical parsers would fare any better , since they are generally trained on corpora containing a totally different genre of text .
however , future work will most likely include a comparison of different parsers .
examination of our results show that a better notion of constituency would increase the accuracy of our results .
our algorithm occasionally generates non-sensical paraphrases that cross constituent boundaries , for example , including the verb of a subordinate clause with elements from the matrix clause .
other problems arise because our current algorithm has no notion of verb phrases ; it often generates near misses such as fail ^ - succeed , neglecting to include not as part of the paraphrase .
however , there are problems inherent in paraphrase generation that simple knowledge of constituency alone cannot solve .
consider the following two sentences : arguably , all three paraphrases are valid , although opinions vary more regarding the last paraphrase .
what is the optimal level of structure for paraphrases ?
obviously , this represents a tradeoff between specificity and accuracy , but the ability of structural paraphrases to capture long-distance relationships across large numbers of lexical items complicates the problem .
due to the sparseness of our data , our algorithm cannot make a good decision on what constituents to generalize as variables ; naturally , greater amounts of data would alleviate this problem .
this current inability to decide on a good scope for paraphrasing was a primary reason why we were unable to perform a strict evaluation of recall .
our initial attempts at generating a gold standard for estimating recall failed because human judges could not agree on the boundaries of paraphrases .
the accuracy of our structural paraphrases is highly dependent on the corpus size .
as can be seen from the numbers in table 1 , paraphrases are rather sparsenearly 93 % of them are unique .
without adequate statistical evidence , validating candidate paraphrases can be very difficult .
although our data spareness problem can be alleviated simply by gathering a larger corpus , the type of parallel text our algorithm requires is rather hard to obtain , i.e. , there are only so many translations of so many foreign novels .
furthermore , since our paraphrases are arguably genre-specific , different applications may require different training corpora .
similar to the work of barzilay and lee ( 2003 ) , who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event , we are currently attempting to solve the data sparseness problem by extending our approach to non-parallel corpora .
we believe that generating paraphrases at the structural level holds several key advantages over lexical paraphrases , from the capturing of long- distance relationships to the more accurate modeling of context .
the paraphrases generated by our approach could prove to be useful in any natural language application where understanding of linguistic variations is important .
in particular , we are attempting to apply our results to improve the performance of question answering system , which we will describe in the following section . 6 paraphrases and question answering the ultimate goal of our work on paraphrases is to enable the development of high-precision question answering system ( cf . ( katz and levin , 1988 ; soubbotin and soubbotin , 2001 ; hermjakob et al. , 2002 ) ) .
we believe that a knowledge base of paraphrases is the key to overcoming challenges presented by the expressiveness of natural languages .
because the same semantic content can be expressed in many different ways , a question answering system must be able to cope with a variety of alternative phrasings .
in particular , an answer stated in a form that differs from the form of the question presents significant problems : in the above examples , question answering systems have little difficulty extracting answers if the answers are stated in a form directly derived from the question , e.g. , ( 1a ) and ( 2a ) ; simple keyword matching techniques with primitive named-entity detection technology will suffice .
however , question answering systems will have a much harder time extracting answers from sentences where they are not obviously stated , e.g. , ( 1b ) and ( 2b ) .
to relate question to answers in those examples , a system would need access to rules like the following : we believe that such rules are best formulated at the syntactic level : structural paraphrases represent a good level of generality and provide much more accurate results than keyword-based approaches .
the simplest approach to overcoming the paraphrase problem in question answering is via keyword query expansion when searching for candidate answers : the major drawback of such techniques is over- generation of bogus answer candidates .
for example , it is a well-known result that query expansion based on synonymy , hyponymy , etc. may actually degrade performance if done in an uncontrolled manner ( voorhees , 1994 ) .
typically , keyword-based query expansion techniques sacrifice significant amounts of precision for little ( if any ) increase in recall .
the problems associated with keyword query expansion techniques stem from the fundamental deficiencies of bag-of-words approaches ; in short , they simply cannot accurately model the semantic content of text , as illustrated by the following pairs of sentences and phrases that have the same word content , but dramatically different meaning : the above examples are nearly indistinguishable in terms of lexical content , yet their meanings are vastly different .
naturally , because one text fragment might be an appropriate answer to a question while the other fragment may not be , a question answering system seeking to achieve high precision must provide mechanisms for differentiating the semantic content of the pairs .
while paraphrase techniques at the keyword-level vastly overgenerate , paraphrase techniques at the phrase-level undergenerate , that is , they are often too specific .
although paraphrase rules can easily be formulated at the string-level , e.g. , using regular expression matching and substitution techniques ( soubbotin and soubbotin , 2001 ; hermjakob et al. , 2002 ) , such a treatment fails to capture important linguistic generalizations .
for example , the addition of an adverb typically does not alter the validity of a paraphrase ; thus , a phrase-level rule x killed y , x ended ys life would not be able to match an answer like john wilkes booth suddenly ended abraham lincolns life with a bullet .
string-level paraphrases are also unable to handle syntactic phenomenona like passivization , which are easily captured at the syntactic level .
we believe that answering questions at level of syntactic relations , that is , matching parsed representations of questions with parsed representa tions of candidates , addresses the issues presented above .
syntactic relations , basically simplified versions of dependency structures derived from the link parser , can capture significant portions of the meaning present in text documents , while providing a flexible foundation on which to build machinery for paraphrases .
our position is that question answering should be performed at the level of key relations in addition to keywords .
we have begun to experiment with relations indexing and matching techniques described above using an electronic encyclopedia as the test corpus .
we identified a particular set of linguistic phenomena where relation-based indexing can dramatically boost the precision of a question answering system ( katz and lin , 2003 ) .
as an example , consider a sample output from a baseline keyword- based ir system : of the 32 sentences returned , only ( r4 ) correctly answers the user query ; the other results answer a different questionwhat eats frogs ?
a bag-ofwords approach fundamentally cannot differentiate between a query in which the frog is in the subject position and a query in which the frog is in the object position .
compare this to the results produced by our relations matcher : by examining subject-verb-object relations , our system can filter out irrelevant results and return only the correct responses .
we are currently working on combining this relations-indexing technology with the automatic paraphrase generation technology described earlier .
for example , our approach would be capable of automatically learning a paraphrase like x eat y y is a prey ofx ; a large collection of such paraphrases would go a long way in overcoming the brittleness associated with a relations-based indexing scheme .
contributions .
we have presented a method for automatically learning structural paraphrases from aligned monolingual corpora that overcomes the limitation of previous approaches .
in addition , we have sketched how this technology can be applied to enhance the performance of a question answering system based on indexing relations .
although we have not completed a task-based evaluation , we believe that the ability to handle variations in language is key to building better question answering systems .
