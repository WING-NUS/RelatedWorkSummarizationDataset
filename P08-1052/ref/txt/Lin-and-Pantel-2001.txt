discovery of inference rules for question answering .
abstract .
one of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts .
while humans appear to use inference rules such as " x writes y " implies " x is the author of y " in answering questions , such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them .
in this paper , we present an unsupervised algorithm for discovering inference rules from text .
our algorithm is based on an extended version of harris ' distributional hypothesis , which states that words that occurred in the same contexts tend to be similar .
instead of using this hypothesis on words , we apply it to paths in the dependency trees of a parsed corpus .
essentially , if two paths tend to link the same set of words , we hypothesize that their meanings are similar .
we use examples to show that our system discovers many inference rules easily missed by humans .
introduction .
one of the main challenges in question-answering and information retrieval is the potential mismatch between the expressions in questions and the expressions in texts .
suppose a question is phrased as " who is the author of the ' star spangled banner ' ? " .
we call " x wrote y ~ x is the author of y an inference rule .
in previous work , such relationships have been referred to as paraphrases or variants ( sparck jones and tait , 1984 ; fabre and jacquemin , 2000 ) .
in this paper , we use the term inference rule because we also want to include relationships that are not exactly paraphrases , but are nonetheless related and are potentially useful to question-answering systems .
for example , " x caused y ~ y is blamed on x " is an inference rule even though the two phrases do not mean exactly the same thing .
inference rules are extremely important in many fields such as natural language processing , information retrieval , and artificial intelligence in general .
in the lasso / falcon systems ( harabagiu et al. , 2000 ) , the most successful qa systems in trec-8 and trec-9 , a theorem prover is used to justify the answers .
falcon scored 19 % higher with the answer justification process than without it .
traditionally , knowledge bases containing such inference rules are created manually .
this knowledge engineering task is extremely laborious .
more importantly , building such a knowledge base is inherently difficult since humans are not good at generating a complete list of rules .
for example , while it is quite trivial to come up with the rule " x wrote y ~ x is the author of y " , it seems hard to dream up a rule like " x manufactures y ~ xs y factory " , which can be used to infer that " chrtien visited peugots newly renovated car factory in the afternoon " contains an answer to the query " what does peugot manufacture ? "
most previous efforts on knowledge engineering have focused on creating tools for helping knowledge engineers transfer their knowledge to machines ( hahn and schnattinger , 1998 ) .
our goal is to automatically discover such rules .
in this paper , we present an unsupervised algorithm , dirt , for discovering inference rules from text .
our algorithm is a generalization of previous algorithms for finding similar words ( hindle , 1990 ; pereira , 1993 ; lin , 1998 ) .
algorithms for finding similar words assume the distributional hypothesis , which states that words that occurred in the same contexts tend to have similar meanings ( harris , 1985 ) .
instead of applying the distributional hypothesis to words , we apply it to paths in dependency trees .
essentially , if two paths tend to link the same sets of words , we hypothesize that their meanings are similar .
since a path represents a binary relationship , we generate an inference rule for each pair of similar paths .
the remainder of this paper is organized as follows .
in the next section , we review previous work .
in section 3 , we define paths in dependency trees and describe their extraction from a parsed corpus .
section 4 presents the dirt system .
a comparison of our system 's output with manually generated paraphrase expressions is shown in section 5 .
finally , we conclude with a discussion of future work .
previous work .
most previous work on variant recognition and paraphrase has been done in the fields of natural language generation , text summarization , and information retrieval .
the generation community focused mainly on rule-based text transformations in order to meet external constraints such as length and readability ( meteer and shaked , 1988 ; iordanskaja et al. , 1991 ; robin , 1994 ; dras , 1997 ) .
dras ( 1999 ) described syntactic paraphrases using a meta-grammar with a synchronous tree adjoining grammar ( tag ) formalism .
in multi-document summarization , paraphrasing is important to avoid redundant statements in a summary .
given a collection of similar sentences ( a theme ) with different wordings , it is difficult to identify similar phrases that report the same fact .
barzilay et al. ( 1999 ) analyzed 200 two-sentence themes from a corpus and extracted seven lexico-syntactic paraphrasing rules .
these rules covered 82 % of syntactic and lexical paraphrases , which cover 70 % of all variants .
the rules are subsequently used to identify common statements in a theme by comparing the predicate-argument structure of the sentences within the theme .
in information retrieval , it is common to identify phrasal terms from queries and generate their variants for query expansion .
it has been shown that such query expansion does promote effective retrieval ( arampatzis et al. , 1998 ; anick and tipirneni , 1999 ) .
morphological variant query expansion was treated by sparck jones and tait ( 1984 ) using a semantic interpreter and by the fastr system ( jacquemin et al. , 1997 ) .
also , jacquemin ( 1999 ) proposed a rule-based system for the recognition of morpho-syntactic variants using morphological and light syntactic features ( e.g. part-of speech and number agreement ) .
motivated by the fact that morpho-syntactic features inadequately separated correct and incorrect variants , fabre and jacquemin ( 2000 ) later extended this model using lexical semantics for obtaining noun-to-verb variants .
the minor modifications to the model increased the recognition precision by 30 % and reduced recognition recall by 10 % .
in ( richardson , 1997 ) , richardson extracted semantic relationships ( e.g. , hypernym , location , material and purpose ) from dictionary definitions using a parser and constructed a semantic network .
he then described an algorithm that uses paths in the semantic network to compute the similarity between words .
in a sense , our algorithm is a dual of richardson 's approach .
while richardson used paths as features to compute the similarity between words , we use words as features to compute the similarity of paths .
many text mining algorithms aim at finding association rules between terms ( lin et al. , 1998 ) .
in contrast , the output of our algorithm is a set of associations between relations .
term associations usually require human interpretation ; however , some of them are considered to be uninterpretable even by humans ( feldman et. al. , 1998 ) .
extraction of paths from dependency trees .
the inference rules discovered by dirt are between paths in dependency trees .
in this section , we first briefly describe the parser used to generate the dependency trees .
then , we describe an algorithm for extracting paths from the trees .
minipar .
minipar1 is a principle-based english parser ( berwick , 1991 ) .
like principar ( lin , 1993 ) , minipar represents its grammar as a network where nodes represent grammatical categories and links represent types of syntactic ( dependency ) relationships .
the grammar network consists of 35 nodes and 59 links .
additional nodes and links are created dynamically to represent subcategories of verbs .
minipar employs a message passing algorithm that essentially implements distributed chart parsing .
instead of maintaining a single chart , each node in the grammar network maintains a chart containing partially built structures belonging to the grammatical category represented by the node .
the grammatical principles are implemented as constraints associated with the nodes and links .
the lexicon in minipar is derived from syntactic features ( parts of speech and subcategorization frames ) in wordnet ( miller , 1990 ) .
with additional proper names , the lexicon contains about 130,000 entries ( in base forms ) .
the lexicon entry of a word lists all possible parts of speech of the word and its subcategorization frames ( if any ) .
the lexical ambiguities are handled by the parser instead of a tagger .
minipar works with a constituency grammar internally , however the output of minipar is a dependency tree .
the conversion is straightforward because all the constituents in the constituency grammar have a head .
figure 1 shows an example dependency tree for the sentence " john found a solution to the problem . "
the links in the diagram represent dependency relationships .
the direction of a link is from the head to the modifier in the relationship .
labels associated with the links represent types of dependency relations .
table 1 lists a subset of the dependency like chart parsers , minipar constructs all possible parses of an input sentence .
however , only the highest ranking parse tree is output .
although the grammar is manually constructed , the selection of the best parse tree is guided by the statistical information obtained by parsing a 1 gb newspaper corpus with minipar .
the statistical ranking of parse trees is based on the following probabilistic model .
the probability of a dependency tree is defined as the product of the probabilities of the dependency relationships in the tree .
formally , given a tree t with root root consisting of d dependency relationships ( headi , relationshipi , modifieri ) , the probability of t is given by : minipar parses newspaper text at about 500 words per second on a pentium-iii 700mhz with 500mb memory .
evaluation with the manually parsed susanne corpus ( sampson , 1995 ) shows that about 89 % of the dependency relationships in minipar outputs are correct .
the recall of minipar output , defined as the percentage of dependency relationships in the susanne corpus that are extracted by minipar , varies a great deal depending on the genre of the input document from 80 % ( novels ) to 87 % ( news reportage ) .
this accuracy is comparable to other broad-coverage english parsers ( collins , 1996 ; charniak , 2000 ) .
paths in dependency trees .
in the dependency trees generated by minipar , prepositions are represented by nodes .
we apply a simple transformation rule to connect the prepositional complement directly to the words modified by the preposition .
we name this direct relationship with the preposition .
figure 2 gives an example for the phrase " solution to the problem " in which the two links in part ( a ) are replaced with a direct link shown in part ( b ) .
after the transformation , each link between two words in a dependency tree represents a direct semantic relationship .
a path allows us to represent indirect semantic relationships between two content words .
we name a path by concatenating dependency relationships and words along the path , excluding the words at the two ends .
for the sentence in figure 1 , the path between john and problem is named : n : subj : v ffind4 v : obj : n 4solution4n : to : n ( meaning " x finds solution to y ~ ) .
in a path , dependency relations that are not slots are called internal relations .
dirt : discovering inference rules from text .
a path is a binary relation between two entities .
in this section , we present an algorithm to automatically discover the inference relations between such binary relations .
the underlying assumption .
most algorithms for computing word similarity from text corpus are based on a principle known as the distributional hypothesis ( harris , 1985 ) .
the idea is that words that tend to occur in the same contexts tend to have similar meanings .
previous efforts differ in their representation of the context and in their formula for computing the similarity between two sets of contexts .
some algorithms use the words that occurred in a fixed window of a given word as its context while others use the dependency relationships of a given word as its context ( lin , 1998 ) .
consider the words duty and responsibility .
there are many contexts in which both of these words can fit .
based on these common contexts , one can statistically determine that duty and responsibility have similar meanings .
in the algorithms for finding word similarity , dependency links are treated as contexts of words .
in contrast , our algorithm for finding inference rules treats the words that fill the slots of a path as a context for the path .
we make an assumption that this is an extension to the distributional hypothesis : extended distributional hypothesis : if two paths tend to occur in similar contexts , the meanings of the paths tend to be similar .
as it can be seen from the table , there are many overlaps between the corresponding slot fillers of the two paths .
by the extended distributional hypothesis , we can then claim that the two paths have similar meanings .
triples .
to compute the path similarity using the extended distributional hypothesis , we need to collect the frequency counts of all paths in a corpus and the slot fillers for the paths .
for each instance of a path p that connects two words w1 and w2 , we increase the frequency counts of the two triples ( p , slotx , w1 ) and ( p , sloty , w2 ) .
we call ( slotx , w1 ) and ( sloty , w2 ) features of path p .
intuitively , the more features two paths share , the more similar they are .
we use a triple database ( a hash table ) to accumulate the frequency counts of all features of all paths extracted from a parsed corpus .
the first column of numbers in figure 3 represents the frequency counts of a word filling a slot of the path and the second column of numbers is the mutual information between a slot and a slot filler .
mutual information measures the strength of the association between a slot and a filler .
we explain mutual information in detail in the next section .
the triple database records the fillers of slotx and sloty separately .
looking at the database , one would be unable to tell which slotx filler occurred with which sloty filler in the corpus .
mutual information .
mutual information is a commonly used measure for the association strength between two words ( church and hanks , 1989 ) .
the mutual information between two events x and y is given by : mi ( x , y ) = log p ( x , y ) | p ( x ) p ( y ) .
mutual information is high when x and y occur together more often than by chance .
mutual information compares two models for predicting the co- occurrence of x and y : one is the mle of the joint probability of x and y and the other is some baseline model .
in equation ( 1 ) , the baseline model assumes that x and y are independent .
note that in information theory , mutual information refers to the mutual information between two random variables rather than between two events as used in this paper .
the mutual information between two random variables x and y is given by : mi ( x , y ) = p ( x , y ) log pr ( x , p ) ) .
the mutual information between two random variables is the weighted average of all possible combinations of events involving the two variables .
a triple involves three events : the path , the slot , and the filler .
equation ( 1 ) defines the mutual information between two events .
alshawi and carter ( 1994 ) generalized equation ( 1 ) to handle three events .
similaritybetween two paths .
once the triple database is created , the similaritybetweentwo paths canbe computedinthe same way thatthe similaritybetweentwo words is computedin ( lin , 1998 ) .
essentially , two paths have highsimilarity ifthere are alarge numberof commonfeatures .
however , not every feature is equally important .
finding the most similar paths .
given a path , the discovery of inference rules is made by finding its most similar paths .
the challenge here is that there are a large number of paths in the triple database .
the database used in our experiments contains over 200,000 distinct paths .
computing the similarity between every pair of paths is obviously impractical .
given a path p , our algorithm for finding the most similar paths of p takes three steps : retrieve all the paths that share at least one feature with p and call them candidate paths .
this can be done efficiently by storing for each word the set of slots it fills in .
for each candidate path c , count the number of features shared by c and p .
filter out c if the number of its common features with p is less than a fixed percent ( we used 1 % ) of the total number of features for p and c .
this step effectively uses a simpler similarity formula to filter out some of the paths since computing mutual information is more costly than counting the number of features .
this idea has previously been used in canopy ( mccallum et al. , 2000 ) .
table 3 lists the top-50 most similar paths to " x solves p " generated by dirt .
the ones tagged with an asterisk ( * ) are incorrect , as judged by the authors .
most of the paths can be considered as paraphrases of the original expression .
the extended distributional hypothesis , as with the original distributional hypothesis , is a statement about general trend instead of individual instances .
there are plenty of exceptions in the text , as evidenced by the asterisk-tagged paths in table 3 .
experimental results .
ideally , we would evaluate our system by injecting the inference rules into a full- fledged question-answering system .
however , at this point , we have not built such a system .
therefore , we performed an evaluation of our algorithm by comparing the inference rules it generates with a set of human-generated paraphrases of the first 15 questions in the trec-8 question-answering track , listed in table 4 .
trec ( text retrievial conference ) is a u.s. government sponsored competition on information retrieval held annually since 1992 .
in the question- answering track , the task for participating systems is to find answers to natural- language questions like those in table 4 .
experimental setup .
we used minipar to parse about 1gb of newspaper text ( san jose mercury , wall street journal and ap newswire from the trec collection ) .
using the methods discussed in section 3 , we extracted 7 million paths from the parse trees ( 231,000 unique ) and stored them in a triple database .
results .
the second column of table 5 shows the paths that minipar identified from the trec-8 questions .
for some questions , more than one path was identified .
for others , no path was found ( represented by 0 in table 5 ) .
we compare the output of our algorithm with a set of manually generated paraphrases of the trec-8 questions made available at isi2 .
table 6 gives the paraphrases for q1 and q3 .
we also extracted paths from the manually generated paraphrases .
for some paraphrases , an identical path is extracted .
for example , " what things are manufactured by peugeot ? " and " what products are manufactured by peugeot ? " both map to the path " x is manufactured by y " .
additionally , some paraphrases do not map to any paths .
for example , one of the paraphrases of q2 is the multi-sentence query " when the norwegian nobel committee issues a peace prize , it also gives a financial award .
how much was that award in 1989 ? "
this paraphrase does not seem to contain any variation of the path in the original question .
the number of paths for the manually generated paraphrases of trec-8 questions is shown in the third column of table 5 .
for each of the paths p in the second column of table 5 , we ran the dirt algorithm to compute its top-40 most similar paths using the triple database .
we then manually inspected the outputs and classified each extracted path as correct or incorrect .
a path p ' is judged correct if a sentence containing p ' might contain an answer to the question from which p was extracted .
consider question q3 in the fourth column in table 5 shows the number of top-40 most similar paths classified as correct and the fifth column gives the intersection between columns three and four .
finally , the last column in table 5 gives the percentage of top-40 paths classified as correct .
observations .
there is very little overlap between the automatically generated paths and the paraphrases , even though the percentage of correct paths in dirt outputs can be quite high .
this suggests that finding potentially useful inference rules is very difficult for humans as well as machines .
table 7 shows some of the correct paths among the top-40 extracted by our system for five of the trec-8 questions .
many of the variations generated by dirt that are correct paraphrases are missing from the manually generated variations , and vice versa .
it is difficult for humans to generate a diverse list of paraphrases , given a starting formulation and no context .
however , given the output of our system , humans can easily identify the correct inference rules .
hence , at the least , our system would greatly ease the manual construction of inference rules for a qa system .
the performance of dirt varies a great deal for different paths .
usually , the performance for paths with verb roots is much better than for paths with noun roots .
a verb phrase typically has more than one modifier3 , whereas nouns usually take a smaller number of modifiers .
when a word takes less than two modifiers , it will not be the root of any path .
as a result , paths with noun roots occur less often than paths with verb roots , which explains the lower performance with respect to paths with noun roots .
in table 5 , dirt found no correct inference rules for three of the questions .
the paths for qz and qg do not have any entries in the triple database .
the performance for q7 is poor for a different reason .
although the triple database contains plenty of features for " x leaves p " , all of the similar paths found by dirt refer to the travel sense of leave , such as " x flees p " and " x visits p " .
in q7 : what debts did qintex group leave ? the intended meaning of leave is " to cause something to remain . "
another source of error in our algorithm is exemplified by the following : among the most similar paths of " x asks p " , we have " x informs p " ( which is correct ) , but also " p asks x " and " p informs x " .
the reason is that both askers and askees tend to be persons or organizations .
since the similarity of paths depends totally on the similarity of their slots , slots with the same kind of fillers are not distinguished in our algorithm .
predicting whether this type of error will happen in the outputs for a given path is easy .
we can simply compute the similarity between its slotx and its sloty .
however , teasing out the incorrect inference rules caused by this is still a problem .
conclusion and future work .
better tools are necessary to tap into the vast amount of textual data that is growing at an astronomical pace .
knowledge about inference relationships between natural language expressions is extremely important for question-answering and many other applications of natural language processing .
to the best of our knowledge , this is the first attempt to discover such knowledge automatically from a large corpus of text .
we introduced the extended distributional hypothesis , which states that paths in dependency trees have similar meanings if they tend to connect similar sets of words .
treating paths as binary relations , our algorithm is able to generate inference rules by searching for similar paths .
our experimental results show that the extended distributional hypothesis can indeed be used to discover very useful inference rules , many of which , though easily recognizable , are difficult for humans to recall .
many questions remain to be addressed .
one is to recognize the polarity in inference relationships .
high similarity values are often assigned to relations with opposite polarity .
in another work ( lin and pantel , 2001 ) , we constructed semantic classes from text corpus with an unsupervised algorithm .
for example , the following are two classes generated by our program ( nq1446 and nq1471 are automatically generated class names ) .
