(claim)#transliteration methods typically fall into two categories : 
0.1#(li et al., 2004);(jung et al., 2000);(knight and graehl, 1998)#generative approaches $1 $2 $3 that try to produce the target transliteration given a source language ne , generative methods encounter the out-of-vocabulary ( oov ) problem and require substantial amounts of training data and knowledge of the source and target languages . 
0.2#(goldwasser and roth, 2008b);(bergsma and kondrak,2007);(sproat et al., 2006);(klementiev and roth, 2006a)#and discriminative approaches $1 $2 $3 $4 , that try to identify the correct transliteration for a word in the source language given several candidates in the target language . discriminative approaches , when used to for discovering ne in a bilingual corpora avoid the oov problem by choosing the transliteration candidates from the corpora .
(claim)#these methods typically make very little assumptions about the source and target languages and require considerably less data to converge .
0.3#(bergsma and kondrak,2007);(goldwasser and roth, 2008b);(sproat et al., 2006);(klementiev and roth, 2006a)#training the transliteration model is typically done under supervised settings $1 $2 , or weakly supervised settings with additional temporal information $3 $4 .
(claim)#our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data .
(claim)#incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the nlp community recently .
0.3#(roth and yih, 2004);(riedel and clarke, 2006);(haghighi and klein, 2006);(chang et al., 2007)#this has been shown both in supervised settings $1 $2 and unsupervised settings $3 $4 in which constraints are used to bootstrap the model .
0.3#(chang et al., 2007)#$1 describes an unsupervised training of a constrained conditional model ( ccm ) , a general framework for combining statistical models with declarative constraints .
(claim)#we extend this work to include constraints over possible assignments to latent variables which , in turn , define the underlying representation for the learning problem .
0.3#(ristad and yianilos, 1998);(bergsma and kondrak,2007);(goldwasser and roth, 2008b)#in the transliteration community there are several works $1 $2 $3 that show how the feature representation of a word pair can be restricted to facilitate learning a string similarity model .
0.3#(goldwasser and roth, 2008b)#we follow the approach discussed in $1 , which considers the feature representation as a structured prediction problem and finds the set of optimal assignments ( or feature activations ) , under a set of legitimacy constraints . this approach stresses the importance of interaction between learning and inference , as the model iteratively uses inference to improve the sample representation for the learning problem and uses the learned model to improve the accuracy of the inference process .
(claim)#we adapt this approach to unsupervised settings , where iterating over the data improves the model in both of these dimensions .
