active sample selection for named entity transliteration .
abstract .
this paper introduces a new method for identifying named-entity ( ne ) transliterations within bilingual corpora .
current state-of-the- art approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages .
we show how to effectively train an accurate transliteration classifier using very little data , obtained automatically .
to perform this task , we introduce a new active sampling paradigm for guiding and adapting the sample selection process .
we also investigate how to improve the classifier by identifying repeated patterns in the training data .
we evaluated our approach using english , russian and hebrew corpora .
introduction .
this paper presents a new approach for constructing a discriminative transliteration model .
our approach is fully automated and requires little knowledge of the source and target languages .
named entity ( ne ) transliteration is the process of transcribing a ne from a source language to a target language based on phonetic similarity between the entities .
figure 1 provides examples of ne transliterations in english russian and hebrew .
identifying transliteration pairs is an important component in many linguistic applications such as machine translation and information retrieval , which require identifying out-of-vocabulary words .
in our settings , we have access to source language ne and the ability to label the data upon request .
we introduce a new active sampling paradigm that aims to guide the learner toward informative samples , allowing learning from a small number of representative examples .
after the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model .
previous works usually take a generative approach , ( knight and graehl , 1997 ) .
other approaches exploit similarities in aligned bilingual corpora ; for example , ( tao et al. , 2006 ) combine two unsupervised methods . ( klementiev and roth , 2006 ) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method .
although these approaches alleviate the problem of obtaining annotated data , other resources are still required , such as a large aligned bilingual corpus .
the idea of selectively sampling training samples has been wildly discussed in machine learning theory ( seung et al. , 1992 ) and has been applied successfully to several nlp applications ( mccallum and nigam , 1998 ) .
unlike other approaches , our approach is based on minimizing the distance between the feature distribution of a comprehensive reference set and the sampled set .
training a transliteration model .
our framework works in several stages , as summarized in algorithm 1 .
first , a training set consisting of ne transliteration pairs ( ws , wt ) is automatically generated using an active sample selection scheme .
the sample selection process is guided by the sufficient spanning features criterion ( ssf ) introduced in section 2.2 , to identify informative samples in the source language.an oracle capable of pairing a ne in the source language with its counterpart in the target language is then used .
negative training samples are generated by reshuffling the terms in these pairs .
once the training data has been collected , the data is analyzed to identify repeating patterns in the data which are used to focus the training process by assigning weights to features corresponding to the observed patterns .
finally , a linear model is trained using a variation of the averaged perceptron ( freund and schapire , 1998 ) algorithm .
the remainder of this section provides details about these stages ; the basic formulation of the transliteration model and the feature extraction scheme is described in section 2.1 , in section 2.2 the selective sampling process is described and finally section 2.3 explains how learning is focused by using feature weights .
transliteration model .
our transliteration model takes a discriminative approach ; the classifier is presented with a word pair ( ws , wt ) , where ws is a named entity and it is asked to determine whether wt is a transliteration of the ne in the target language .
we use a linear classifier trained with a regularized perceptron update rule ( grove and roth , 2001 ) as implemented in snow , ( roth , 1998 ) .
the classifier 's confidence score is used for ranking of positively tagged transliteration candidates .
our initial feature extraction scheme follows the one presented in ( klementiev and roth , 2006 ) , in which the feature space consists of n-gram pairs from the two languages .
given a sample , each word is decomposed into a set of sub-strings of up to a given length ( including the empty string ) .
features are generated by pairing substrings from the two sets whose relative positions in the original words differ by one or less places ; first each word is decomposed into a set of substrings then substrings from the two sets are coupled to complete the pair representation .
figure 2 depicts this process .
guiding the sampling process with ssf .
the initial step in our framework is to generate a training set of transliteration pairs ; this is done by pairing highly informative source language candidate nes with target language counterparts .
we developed a criterion for adding new samples , sufficiently spanning features ( ssf ) , which quantifies the sampled set ability to span the feature space .
this is done by evaluating the l-1 distance between the frequency distributions of source language word fragments in the current sampled set and in a comprehensive set of source language nes , serving as reference .
we argue that since the features used for learning are n-gram features , once these two distributions are close enough , our examples space provides a good and concise characterization of all named entities we will ever need to consider .
a special care should be given to choosing an appropriate reference ; as a general guideline the reference set should be representative of the testing data .
we collected a set r , consisting of 50,000 ne by crawling through wikipedia 's articles and using an english ner system available at - http : / / l2r.cs.uiuc.edu / cogcomp .
the frequency distribution was generated over all character level bi-grams appearing in the text , as bi-grams best correlate with the way features are extracted .
transliteration oracle implementation .
the transliteration oracle is essentially a mapping between the named entities , i.e. given an ne in the source language it provides the matching ne in the target language .
an automatic oracle was implemented by crawling through wikipedia topic aligned document pairs .
given a pair of topic aligned documents in the two languages , the topic can be identified either by identifying the top ranking terms or by simply identifying the title of the documents .
by choosing documents in wikipedia 's biography category we ensured that the topic of the documents is person ne .
training the transliteration model .
the feature extraction scheme we use generates features by coupling substrings from the two terms .
ideally , given a positive sample , it is desirable that paired substrings would encode phonetically similar or a distinctive context in which the two scripts correlate .
given enough positive samples , such features will appear with distinctive frequency .
taking this idea further , these features were recognized by measuring the co-occurrence frequency of sub- strings of up to two characters in both languages .
each feature f = ( fs , ft ) composed of two substrings taken from english and hebrew words was associated with weight .
evaluation .
we evaluated our approach in two settings ; first , we compared our system to a baseline system described in ( klementiev and roth , 2006 ) .
given a bilingual corpus with the english ne annotated , the system had to discover the ne in target language text .
we used the english-russian news corpus used in the baseline system .
nes were grouped into equivalence classes , each containing different variations of the same ne .
we randomly sampled 500 documents from the corpus .
transliteration pairs were mapped into 97 equivalence classes , identified by an expert .
in a second experiment , different learning parameters such as selective sampling efficiency and feature weights were checked . 300 english-russian and english-hebrew ne pairs were used ; negative samples were generated by coupling every english ne with all other target language nes .
using ssf directed sampling .
table 2 describes the effect of directed sampling in the english-russian news corpora ne discovery task .
results show that models trained using selective sampling can outperform models trained with more than twice the amount of data .
training using feature weights .
table 3 describes the effect training the model with weights .
the training set consisted of 150 samples extracted using ssf directed sampling .
three variations were tested - training without feature weights , using the feature weights as the initial network weights without training and training with weights .
the results clearly show that using weights for training improve the classifier 's performance for both russian and hebrew .
it can also be observed that in many cases the correct pair was ranked in any of the top five places .
conclusions and future work .
in this paper we presented a new approach for constructing a transliteration model automatically and efficiently by selectively extracting transliteration samples covering relevant parts of the feature space and focusing the learning process on these features .
we show that our approach can outperform systems requiring supervision , manual intervention and a considerable amount of data .
we propose a new measure for selective sample selection which can be used independently .
we currently investigate applying it in other domains with potentially larger feature space than used in this work .
another aspect investigated is using our selective sampling for adapting the learning process for data originating from different sources using the a reference set representative of the testing data , training samples , originating from a different source , can be biased towards the testing data .
