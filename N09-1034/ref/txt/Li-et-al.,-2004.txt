a joint source-channel model for machine transliteration .
abstract .
most foreign names are transliterated into chinese , japanese or korean with approximate phonetic equivalents .
the transliteration is usually achieved through intermediate phonemic mapping .
this paper presents a new framework that allows direct orthographical mapping ( dom ) between two different languages , through a joint source-channel model , also called n-gram transliteration model ( tm ) .
with the n-gram tm model , we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary .
the n-gram tm under the dom framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms .
the modeling framework is validated through several experiments for english-chinese language pair .
introduction .
in applications such as cross-lingual information retrieval ( clir ) and machine translation , there is an increasing need to translate out-of-vocabulary words from one language to another , especially from alphabet language to chinese , japanese or korean .
proper names of english , french , german , russian , spanish and arabic origins constitute a good portion of out-of-vocabulary words .
they are translated through transliteration , the method of translating into another language by preserving how words sound in their original languages .
for writing foreign names in chinese , transliteration always follows the original romanization .
therefore , any foreign name will have only one pinyin ( romanization of chinese ) and thus in chinese characters .
in this paper , we focus on automatic chinese transliteration of foreign alphabet names .
because some alphabet writing systems use various diacritical marks , we find it more practical to write names containing such diacriticals as they are rendered in english .
therefore , we refer all foreign-chinese transliteration to english-chinese transliteration , or e2c .
transliterating english names into chinese is not straightforward .
however , recalling the original from chinese transliteration is even more challenging as the e2c transliteration may have lost some original phonemic evidences .
the chinese-english backward transliteration process is also called back-transliteration , or c2e ( knight & graehl , 1998 ) .
in machine transliteration , the noisy channel model ( ncm ) , based on a phoneme-based approach , has recently received considerable attention ( meng et al. 2001 ; jung et al , 2000 ; virga & khudanpur , 2003 ; knight & graehl , 1998 ) .
in this paper we discuss the limitations of such an approach and address its problems by firstly proposing a paradigm that allows direct orthographic mapping ( dom ) , secondly further proposing a joint source-channel model as a realization of dom .
two other machine learning techniques , ncm and id3 ( quinlan , 1993 ) decision tree , also are implemented under dom as reference to compare with the proposed n-gram tm .
this paper is organized as follows : in section 2 , we present the transliteration problems .
in section 3 , a joint source-channel model is formulated .
in section 4 , several experiments are carried out to study different aspects of proposed algorithm .
in section 5 , we relate our algorithms to other reported work .
finally , we conclude the study with some discussions .
problems in transliteration .
transliteration is a process that takes a character string in source language as input and generates a character string in the target language as output .
the process can be seen conceptually as two levels of decoding : segmentation of the source string into transliteration units ; and relating the source language transliteration units with units in the target language , by resolving different combinations of alignments and unit mappings .
a unit could be a chinese character or a monograph , a digraph or a trigraph and so on for english .
phoneme-based approach .
the problems of english-chinese transliteration have been studied extensively in the paradigm of noisy channel model ( ncm ) .
for a given english name e as the observed channel output , one seeks a posteriori the most likely chinese transliteration c that maximizes p ( ci e ) .
applying bayes rule , it means to find c to maximize p ( e , c ) = p ( e | c ) * p ( c ) ( 1 ) with equivalent effect .
to do so , we are left with modeling two probability distributions : p ( ei c ) , the probability of transliterating c to e through a noisy channel , which is also called transformation rules , and p ( c ) , the probability distribution of source , which reflects what is considered good chinese transliteration in general .
likewise , in c2e back- transliteration , we would find e that maximizes p ( e , c ) = p ( c | e ) * p ( e ) ( 2 ) for a given chinese name .
in eqn ( 1 ) and ( 2 ) , p ( c ) and p ( e ) are usually estimated using n-gram language models ( jelinek , 1991 ) .
inspired by research results of grapheme-tophoneme research in speech synthesis literature , many have suggested phoneme-based approaches to resolving p ( eic ) and p ( cie ) , which approximates the probability distribution by introducing a phonemic representation .
in this way , we convert the names in the source language , say e , into an intermediate phonemic representation p , and then convert the phonemic representation into the target language , say chinese c. in e2c transliteration , the phoneme-based approach can be formulated as p ( cie ) = p ( cip ) p ( pie ) and conversely we have p ( ei c ) = p ( eip ) p ( pi c ) for c2e back-transliteration .
several phoneme-based techniques have been proposed in the recent past for machine transliteration using transformation-based learning algorithm ( meng et al. 2001 ; jung et al , 2000 ; virga & khudanpur , 2003 ) and using finite state transducer that implements transformation rules ( knight & graehl , 1998 ) , where both handcrafted and data-driven transformation rules have been studied .
however , the phoneme-based approaches are limited by two major constraints , which could compromise transliterating precision , especially in english-chinese transliteration : latin-alphabet foreign names are of different origins .
for instance , french has different phonic rules from those of english .
the phoneme-based approach requires derivation of proper phonemic representation for names of different origins .
one may need to prepare multiple language-dependent grapheme-to-phoneme ( g2p ) conversion systems accordingly , and that is not easy to achieve ( the onomastica consortium , 1995 ) .
for example , / lafontant / is transliterated into t , t4- ) ft ( la-fengtang ) while / constant / becomes * w ( kangsi-tan-te ) , where syllable / -tant / in the two names are transliterated differently depending on the names ï¿½ language of origin .
suppose that language dependent graphemeto-phoneme systems are attainable , obtaining chinese orthography will need two further steps : a ) conversion from generic phonemic representation to chinese pinyin ; b ) conversion from pinyin to chinese characters .
each step introduces a level of imprecision .
virga and khudanpur ( 2003 ) reported 8.3 % absolute accuracy drops when converting from pinyin to chinese characters , due to homophone confusion .
unlike japanese katakana or korean alphabet , chinese characters are more ideographic than phonetic .
to arrive at an appropriate chinese transliteration , one cannot rely solely on the intermediate phonemic representation .
useful orthographic context .
however , a possible segmentation / min-ah-an / could lead to an undesirable syllabication of haraj- _ c ( pinyin : min-a-an ) .
according to the transliteration guidelines , a wise segmentation can be reached only after exploring the combination of the left and right context of transliteration units .
from the computational point of view , this strongly suggests using a contextual n-gram as the knowledge base for the alignment decision .
another example will show us how one-to-many mappings could be resolved by context .
let 's take another name / smith / as an example .
although we can arrive at an obvious segmentation / s-mi-th / , there are three chinese characters for each of / s- / , / -mi- / and / -th / .
furthermore , / s- / and / -th / correspond to overlapping characters as well , as shown next .
a human translator will use transliteration rules between english syllable sequence and chinese character sequence to obtain the best mapping ^ ^ - ^ , as indicated in italic in the table above .
to address the issues in transliteration , we propose a direct orthographic mapping ( dom ) framework through a joint source-channel model by fully exploring orthographic contextual information , aiming at alleviating the imprecision introduced by the multiple-step phoneme-based approach .
joint source-channel model .
in view of the close coupling of the source and target transliteration units , we propose to estimate p ( e , c ) by a joint source-channel model , or n-gram transliteration model ( tm ) .
for k aligned transliteration units , we have which provides an alternative to the phoneme- based approach for resolving eqn . ( 1 ) and ( 2 ) by eliminating the intermediate phonemic representation .
unlike the noisy-channel model , the joint source-channel model does not try to capture how source names can be mapped to target names , but rather how source and target names can be generated simultaneously .
in other words , we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both transliteration and back-transliteration .
suppose that we have an english name chinese characters .
oftentimes , the number of letters is different from the number of chinesecharacters .
a chinese character may correspond to a letter substring in english or vice versa .
transliteration alignment .
a bilingual dictionary contains entries mapping english names to their respective chinese transliterations .
like many other solutions in computational linguistics , it is possible to automatically analyze the bilingual dictionary to acquire knowledge in order to map new english names to chinese and vice versa .
based on the transliteration formulation above , a transliteration model can be built with transliteration unit 's n- gram statistics .
to obtain the statistics , the bilingual dictionary needs to be aligned .
the maximum likelihood approach , through em algorithm ( dempster , 1977 ) , allows us to infer such an alignment easily as described in the table below .
the aligning process is different from that of transliteration given in eqn . ( 4 ) or ( 5 ) in that , here we have fixed bilingual entries , ^ and ^ .
the aligning process is just to find the alignment segmentation ^ between the two strings that maximizes the joint probability : a set of transliteration pairs that is derived from the aligning process forms a transliteration table , which is in turn used in the transliteration decoding .
as the decoder is bounded by this table , it is important to make sure that the training database covers as much as possible the potential transliteration patterns .
here are some examples of resulting alignment pairs .
knowing that the training data set will never be sufficient for every n-gram unit , different smoothing approaches are applied , for example , by using backoff or class-based models , which can be found in statistical language modeling literatures ( jelinek , 1991 ) .
dom : n-gram tm vs. ncm .
although in the literature , most noisy channel models ( ncm ) are studied under phoneme-based paradigm for machine transliteration , ncm can also be realized under direct orthographic mapping ( dom ) .
next , let 's look into a bigram case to see what n-gram tm and ncm present to us .
for e2c conversion , re-writing eqn ( 1 ) and eqn ( 6 ) , we have description than that of ncm .
the actual size of models largely depends on the availability of training data .
in table 1 , one can get an idea of how they unfold in a real scenario .
with adequately sufficient training data , n-gram tm is expected to outperform ncm in the decoding .
a perplexity study in section 4.1 will look at the model from another perspective .
the experiments .
we use a database from the bilingual dictionary ï¿½ chinese transliteration of foreign personal names ï¿½ which was edited by xinhua news agency and was considered the de facto standard of personal name transliteration in today 's chinese press .
the database includes a collection of 37,694 unique english entries and their official chinese transliteration .
the listing includes personal names of english , french , spanish , german , arabic , russian and many other origins .
the database is initially randomly distributed into 13 subsets .
in the open test , one subset is withheld for testing while the remaining 12 subsets are used as the training materials .
this process is repeated 13 times to yield an average result , which is called the 13-fold open test .
after experiments , we found that each of the 13-fold open tests gave consistent error rates with less than 1 % deviation .
therefore , for simplicity , we randomly select one of the 13 subsets , which consists of 2896 entries , as the standard open test set to report results .
in the close test , all data entries are used for training and testing .
modeling .
the alignment of transliteration units is done fully automatically along with the n-gram tm training process .
to model the boundary effects , we introduce two extra units < s > and < / s > for start and end of each name in both languages .
the em iteration converges at 8th round when no further alignment changes are reported .
next are some statistics as a result of the model training : the most common metric for evaluating an n- gram model is the probability that the model assigns to test data , or perplexity ( jelinek , 1991 ) .
for a test set w composed of v names , where each name has been aligned into a sequence of transliteration pair tokens , we can calculate the probability of test set number of aligned transliteration pair tokens in the data w. the perplexity ppp ( w ) of a model is the reciprocal of the average probability assigned by the model to each aligned pair in the test set w as ppp ( w ) = 2 h ( w ) .
clearly , lower perplexity means that the model describes better the data .
it is easy to understand that closed test always gives lower perplexity than open test .
we have the perplexity reported in table 2 on the aligned bilingual dictionary , a database of 119,364 aligned tokens .
the ncm perplexity is computed using n-gram equivalents of eqn . ( 8 ) for e2c transliteration , while tm perplexity is based on those of eqn ( 9 ) which applies to both e2c and c2e .
it is shown that tm consistently gives lower perplexity than ncm in open and closed tests .
we have good reason to expect tm to provide better transliteration results which we expect to be confirmed later in the experiments .
the viterbi algorithm produces the best sequence by maximizing the overall probability , p ( ^ , ^ , ^ ) .
in clir or multilingual corpus alignment ( virga and khudanpur , 2003 ) , n-best results will be very helpful to increase chances of correct hits .
in this paper , we adopted an n-best stack decoder ( schwartz and chow , 1990 ) in both tm and ncm experiments to search for n-best results .
the algorithm also allows us to apply higher order n-gram such as trigram in the search .
e2c transliteration .
in this experiment , we conduct both open and closed tests for tm and ncm models under dom paradigm .
results are reported in table 3 and table 4 .
in word error report , a word is considered correct only if an exact match happens between transliteration and the reference .
the character error rate is the sum of deletion , insertion and where wt is the total substitution errors .
only the top choice in n-best results is used for error rate reporting .
not surprisingly , one can see that n-gram tm , which benefits from the joint source-channel model coupling both source and target contextual information into the model , is superior to ncm in all the test cases .
c2e back-transliteration .
the c2e back-transliteration is more challenging than e2c transliteration .
not many studies have been reported in this area .
it is common that multiple english names are mapped into the same chinese transliteration .
in table 1 , we see only 28,632 unique chinese transliterations exist for 37,694 english entries , meaning that some phonemic evidence is lost in the process of transliteration .
to better understand the task , let 's compare the complexity of the two languages presented in the bilingual dictionary .
table 1 also shows that the 5,640 transliteration pairs are cross mappings between 3,683 english and 374 chinese units .
in order words , on average , for each english unit , we have 1.53 = 5,640 / 3,683 chinese correspondences .
in contrast , for each chinese unit , we have 15.1 = 5,640 / 374 english back-transliteration units !
confusion is increased tenfold going backward .
the difficulty of back-transliteration is also reflected by the perplexity of the languages as in table 5 .
based on the same alignment tokenization , we estimate the monolingual language perplexity for chinese and english independently using the n-gram language models. surprise , chinese names have much lower perplexity than english names thanks to fewer chinese units .
this contributes to the success of e2c but presents a great challenge to c2e back- transliteration .
a back-transliteration is considered correct if it falls within the multiple valid orthographically correct options .
experiment results are reported in table 6 .
as expected , c2e error rate is much higher than that of e2c .
in this paper , the n-gram tm model serves as the sole knowledge source for transliteration .
however , if secondary knowledge , such as a lookup table of valid target transliterations , is available , it can help reduce error rate by discarding invalid transliterations top-down the n choices .
in table 7 , the word error rates for both e2c and c2e are reported which imply potential error reduction by secondary knowledge source .
the n-best error rates are reduced significantly at 10-best level as reported in table 7 .
discussions .
it would be interesting to relate n-gram tm to other related framework .
dom : n-gram tm vs id3 .
in section 4 , one observes that contextual information in both source and target languages is essential .
to capture them in the modeling , one could think of decision tree , another popular machine learning approach .
under the dom framework , here is the first attempt to apply decision tree in e2c and c2e transliteration .
with the decision tree , given a fixed size learning vector , we used top-down induction trees to predict the corresponding output .
here we implement id3 ( quinlan , 1993 ) algorithm to construct the decision tree which contains questions and return values at terminal nodes .
similar to n-gram tm , for unseen names in open test , id3 has backoff smoothing , which lies on the default case which returns the most probable value as its best guess for a partial tree path according to the learning set .
in the case of e2c transliteration , we form a learning vector of 6 attributes by combining 2 left and 2 right letters around the letter of focus ek and 1 previous chinese unit ck ^ 1 .
the process is illustrated in table 8 , where both english and chinese contexts are used to infer a chinese character .
similarly , 4 attributes combining 1 left , 1 centre and 1 right chinese character and 1 previous english unit are used for the learning vector in c2e test .
an aligned bilingual dictionary is needed to build the decision tree .
to minimize the effects from alignment variation , we use the same alignment results from section 4 .
two trees are built for two directions , e2c and c2e .
the results are compared with those 3-gram tm in table 9 .
one observes that n-gram tm consistently outperforms id3 decision tree in all tests .
three factors could have contributed : english transliteration unit size ranges from 1 letter to 7 letters .
the fixed size windows in id3 obviously find difficult to capture the dynamics of various ranges. n-gram tm seems to have better captured the dynamics of transliteration units ; the backoff smoothing of n-gram tm is more effective than that of id3 ; unlike n-gram tm , id3 requires a separate aligning process for bilingual dictionary .
the resulting alignment may not be optimal for tree construction .
nevertheless , id3 presents another successful implementation of dom framework .
dom vs. phoneme-based approach .
due to lack of standard data sets , it is difficult to compare the performance of the n-gram tm to that of other approaches .
for reference purpose , we list some reported studies on other databases of e2c transliteration tasks in table 10 .
as in the references , only character and pinyin error rates are reported , we only include our character and pinyin error rates for easy reference .
the reference data are extracted from table 1 and 3 of ( virga and khudanpur 2003 ) .
as we have not found any c2e result in the literature , only e2c results are compared here .
the first 4 setups by virga et al all adopted the phoneme-based approach in the following steps : english name to english phonemes ; english phonemes to chinese pinyin ; 3 ) chinese pinyin to chinese characters .
it is obvious that the n-gram tm compares favorably to other techniques. n-gram tm presents an error reduction of 74.6 % = ( 42.5-10.8 ) / 42.5 % for pinyin over the best reported result , huge mt ( big mt ) test case , which is noteworthy .
the dom framework shows a quantum leap in performance with n-gram tm being the most successful implementation .
the n-gram tm and id3 under direct orthographic mapping ( dom ) paradigm simplify the process and reduce the chances of conversion errors .
as a result , n-gram tm and id3 do not generate chinese pinyin as intermediate results .
it is noted that in the 374 legitimate chinese characters for transliteration , character to pinyin mapping is unique while pinyin to character mapping could be one to many .
since we have obtained results in character already , we expect less pinyin error than character error should a character-to-pinyin mapping be needed .
conclusions .
in this paper , we propose a new framework ( dom ) for transliteration. n-gram tm is a successful realization of dom paradigm .
it generates probabilistic orthographic transformation rules using a data driven approach .
by skipping the intermediate phonemic interpretation , the transliteration error rate is reduced significantly .
furthermore , the bilingual aligning process is integrated into the decoding process in n-gram tm , which allows us to achieve a joint optimization of alignment and transliteration automatically .
unlike other related work where pre-alignment is needed , the new framework greatly reduces the development efforts of machine transliteration systems .
although the framework is implemented on an english-chinese personal name data set , without loss of generality , it well applies to transliteration of other language pairs such as english / korean and english / japanese .
