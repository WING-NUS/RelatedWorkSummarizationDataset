prototype-driven learning for sequence models .
abstract .
we investigate prototype-driven learning for primarily unsupervised sequence modeling .
prior knowledge is specified declaratively , by providing a few canonical examples of each target annotation label .
this sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model .
on part-of-speech induction in english and chinese , as well as an information extraction task , prototype features provide substantial error rate reductions over competitive baselines and outperform previous work .
for example , we can achieve an english part-of-speech tagging accuracy of 80.5 % using only three examples of each tag and no dictionary constraints .
we also compare to semi-supervised learning and discuss the system � s error trends .
introduction .
learning , broadly taken , involves choosing a good model from a large space of possible models .
in supervised learning , model behavior is primarily determined by labeled examples , whose production requires a certain kind of expertise and , typically , a substantial commitment of resources .
in unsupervised learning , model behavior is largely determined by the structure of the model .
designing models to exhibit a certain target behavior requires another , rare kind of expertise and effort .
unsupervised learning , while minimizing the usage of labeled data , does not necessarily minimize total effort .
we therefore consider here how to learn models with the least effort .
in particular , we argue for a certain kind of semi-supervised learning , which we call prototype-driven learning .
in prototype-driven learning , we specify prototypical examples for each target label or label configuration , but do not necessarily label any documents or sentences .
for example , when learning a model for penn treebank-style part-of-speech tagging in english , we may list the 45 target tags and a few examples of each tag ( see figure 4 for a concrete prototype list for this task ) .
this manner of specifying prior knowledge about the task has several advantages .
first , is it certainly compact ( though it remains to be proven that it is effective ) .
second , it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy ( compare , for example , with the list in figure 2 , which suggests an entirely different task ) .
indeed , prototype lists have been used pedagogically to summarize tagsets to students ( manning and sch � utze , 1999 ) .
finally , natural language does exhibit proform and prototype effects ( radford , 1988 ) , which suggests that learning by analogy to prototypes may be effective for language tasks .
in this paper , we consider three sequence modeling tasks : part-of-speech tagging in english and chinese and a classified ads information extraction task .
our general approach is to use distributional similarity to link any given word to similar prototypes .
for example , the word reported may be linked to said , which is in turn a prototype for the part-of-speech vbd .
we then encode these prototype links as features in a log-linear generative model , which is trained to fit unlabeled data ( see section 4.1 ) .
distributional prototype features provide substantial error rate reductions on all three tasks .
for example , on english part-of-speech tagging with three prototypes per tag , adding prototype features to the baseline raises per-position accuracy from 41.3 % to 80.5 % .
tasks and related work : tagging .
for our part-of-speech tagging experiments , we used data from the english and chinese penn treebanks ( marcus et al. , 1994 ; ircs , 2002 ) .
example sentences are shown in figure 1 ( a ) and ( b ) .
a great deal of research has investigated the unsupervised and semi- supervised induction of part-of-speech models , especially in english , and there is unfortunately only space to mention some highly related work here .
one approach to unsupervised learning of partof-speech models is to induce hmms from unlabeled data in a maximum-likelihood framework .
for example , merialdo ( 1991 ) presents experiments learning hmms using em .
merialdo � s results most famously show that re-estimation degrades accuracy unless almost no examples are labeled .
less famously , his results also demonstrate that re- estimation can improve tagging accuracies to some degree in the fully unsupervised case .
one recent and much more successful approach to part-of-speech learning is contrastive estimation , presented in smith and eisner ( 2005 ) .
they utilize task-specific comparison neighborhoods for part-ofspeech tagging to alter their objective function .
both of these works require specification of the legal tags for each word .
such dictionaries are large and embody a great deal of lexical knowledge .
a prototype list , in contrast , is extremely compact .
tasks and related work : extraction .
grenager et al. ( 2005 ) presents an unsupervised approach to an information extraction task , called classifieds here , which involves segmenting classified advertisements into topical sections ( see figure 1 ( c ) ) .
labels in this domain tend to be � sticky � in that the correct annotation tends to consist of multi-element fields of the same label .
the overall approach of grenager et al. ( 2005 ) typifies the process involved in fully unsupervised learning on new domain : they first alter the structure of their hmm so that diagonal transitions are preferred , then modify the transition structure to explicitly model boundary tokens , and so on .
given enough refinements the model learns to segment with a reasonable match to the target structure .
in section 5.3 , we discuss an approach to this task which does not require customization of model structure , but rather centers on feature engineering .
approach .
in the present work , we consider the problem of learning sequence models over text .
for each document x = [ xi ] , we would like to predict a sequence of labels y = [ yi ] , where xi e x and yi e y. we construct a generative model , p ( x , y10 ) , where 0 are the model � s parameters , and choose parameters to maximize the log-likelihood of our observed data d : markov random fields .
note that the only way an mrf differs from a conditional random field ( crf ) ( lafferty et al. , 2001 ) is that the partition function is no longer observation dependent ; we are modeling the joint probability of x and y instead of y given x .
as a result , learning an mrf is slightly harder than learning a crf ; we discuss this issue in section 4.4 .
prototype-driven learning .
we assume prior knowledge about the target structure via a prototype list , which specifies the set of target labels y and , for each label y e y , a set of prototypes words , py e py .
see figures 2 and 4 for examples of prototype lists .
broadly , we would like to learn sequence models which both explain the observed data and meet our prior expectations about target structure .
a straightforward way to implement this is to constrain each prototype word to take only its given label ( s ) at training time .
as we show in section 5 , this does not work well in practice because this constraint on the model is very sparse .
in providing a prototype , however , we generally mean something stronger than a constraint on that word .
in particular , we may intend that words which are in some sense similar to a prototype generally be given the same label ( s ) as that prototype .
distributional similarity .
in syntactic distributional clustering , words are grouped on the basis of the vectors of their preceeding and following words ( sch � utze , 1995 ; clark , 2001 ) .
the underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness ( radford , 1988 ) .
we present more details in section 5 , but for now assume that a similarity function over word types is given .
suppose further that for each non-prototype word type w , we have a subset of prototypes , 5 , , ,, which are known to be distributionally similar to w ( above some threshold ) .
we would like our model to relate the tags of w to those of 5 , , , .
one approach to enforcing the distributional assumption in a sequence model is by supplementing the training objective ( here , data likelihood ) with a penalty term that encourages parameters for which each w � s posterior distribution over tags is compatible with its prototypes 5 .
instead , we introduce distributional prototypes into the learning process as features in our log-linear model .
parameter estimation .
so far we have ignored the issue of how we learn model parameters 0 which maximize l ( 0 ; d ) .
if our model family were hmms , we could use the em algorithm to perform a local search .
since we have a log-linear formulation , we instead use a gradient- based search .
in particular , we use l-bfgs ( liu and nocedal , 1989 ) , a standard numerical optimization technique , which requires the ability to evaluate l ( 0 ; d ) and its gradient at a given 0 .
the density p ( x | 0 ) is easily calculated up to the global constant z ( 0 ) using the forward-backward algorithm ( rabiner , 1989 ) .
as regularization , we use a diagonal gaussian prior with variance q2 = 0.5 , which gave relatively good performance on all tasks .
experiments .
we experimented with prototype-driven learning in three domains : english and chinese part-of-speech tagging and classified advertisement field segmentation .
at inference time , we used maximum posterior decoding , 2 which we found to be uniformly but slightly superior to viterbi decoding .
english pos tagging .
for our english part-of-speech tagging experiments , we used the wsj portion of the english penn treebank ( marcus et al. , 1994 ) .
we took our data to be either the first 48k tokens ( 2000 sentences ) or 193k tokens ( 8000 sentences ) starting from section 2 .
we used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in smith and eisner ( 2005 ) : exact word type , character suffixes of length up to 3 , initial-capital , contains-hyphen , and contains-digit .
our only edge features were tag trigrams .
with just these features ( our baseline base ) the problem is symmetric in the 45 model labels .
in order to break initial symmetry we initialized our potentials to be near one , with some random noise .
to evaluate in this setting , model labels must be mapped to target labels .
we followed the common approach in the literature , greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset .
the results of base , reported in table 1 , depend upon random initialization ; averaging over 10 runs gave an average per-position accuracy of 41.3 % on the larger training set .
we automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often .
this resulted in 116 prototypes for the 193k token setting.3 for comparison , there are 18,423 word types occurring in this data .
incorporating the prototype list in the simplest possible way , we fixed prototype occurrences in the data to their respective annotation labels .
in this case , the model is no longer symmetric , and we no longer require random initialization or post-hoc mapping of labels .
adding prototypes in this way gave an accuracy of 68.8 % on all tokens , but only 47.7 % on non-prototype occurrences , which is only a marginal improvement over base .
it appears as though the prototype information is not spreading to non-prototype words .
in order to remedy this , we incorporated distributional similarity features .
similar to ( sch � utze , 1995 ) , we collect for each word type a context vector of the counts of the most frequent 500 words , conjoined with a direction and distance ( e.g + 1 , -2 ) .
we then performed an svd on the matrix to obtain a reduced rank approximation .
we used the dot product between left singular vectors as a measure of distributional similarity .
for each word w , we find the set of prototype words with similarity exceeding a fixed threshold of 0.35 .
for each of these prototypes z , we add a predicate proto = z to each occurrence of w .
for example , we might add proto = said to each token of reported ( as in figure 3 ) .
each prototype word is also its own prototype ( since a word has maximum similarity to itself ) , so when we lock the prototype to a label , we are also pushing all the words distributionally similar to that prototype towards that label .
this setting , proto + sim , brings the all-tokens accuracy up to 80.5 % , which is a 37.5 % error reduction over proto .
for non-prototypes , the accuracy increases to 67.8 % , an error reduction of 38.4 % over proto .
the overall error reduction from base to proto + sim on all-token accuracy is 66.7 % .
table 5 lists the most common confusions for proto + sim .
the second , third , and fourth most common confusions are characteristic of fully supervised taggers ( though greater in number here ) and are difficult .
for instance , both jjs and nns tend to occur after determiners and before nouns .
the cd and dt confusion is a result of our prototype list not containing a contains-digit prototype for cd , so the predicate fails to be linked to cds .
of course in a realistic , iterative design setting , we could have altered the prototype list to include a contains-digit prototype for cd and corrected this confusion .
figure 6 shows the marginal posterior distribution over label pairs ( roughly , the bigram transition matrix ) according to the treebank labels and the proto + sim model run over the training set ( using a collapsed tag set for space ) .
note that the broad structure is recovered to a reasonable degree .
it is difficult to compare our results to other systems which utilize a full or partial tagging dictionary , since the amount of provided knowledge is substantially different .
the best comparison is to smith and eisner ( 2005 ) who use a partial tagging dictionary .
in order to compare with their results , we projected the tagset to the coarser set of 17 that they used in their experiments .
on 24k tokens , our proto + sim model scored 82.2 % .
when smith and eisner ( 2005 ) limit their tagging dictionary to words which occur at least twice , their best performing neighborhood model achieves 79.5 % .
while these numbers seem close , for comparison , their tagging dictionary contained information about the allowable tags for 2,125 word types ( out of 5,406 types ) and the their system must only choose , on average , between 4.4 tags for a word .
our prototype list , however , contains information about only 116 word types and our tagger must on average choose between 16.9 tags , a much harder task .
when smith and eisner ( 2005 ) include tagging dictionary entries for all words in the first half of their 24k tokens , giving tagging knowledge for 3,362 word types , they do achieve a higher accuracy of 88.1 % .
chinese pos tagging .
we also tested our pos induction system on the chinese pos data in the chinese treebank ( ircs , 2002 ) .
the model is wholly unmodified from the english version except that the suffix features are removed since , in chinese , suffixes are not a reliable indicator of part-of-speech as in english ( tseng et al. , 2005 ) .
since we did not have access to a large auxiliary unlabeled corpus that was segmented , our distributional model was built only from the treebank text , and the distributional similarities are presumably degraded relative to the english .
on 60k word tokens , base gave an accuracy of 34.4 , proto gave 39.0 , and proto + sim gave 57.4 , similar in order if not magnitude to the english case .
we believe the performance for chinese pos tagging is not as high as english for two reasons : the general difficulty of chinese pos tagging ( tseng et al. , 2005 ) and the lack of a larger segmented corpus from which to build distributional models .
nonetheless , the addition of distributional similarity features does reduce the error rate by 35 % from base .
information field segmentation .
we tested our framework on the classifieds data described in grenager et al. ( 2005 ) under conditions similar to pos tagging .
an important characteristic of this domain ( see figure 1 ( a ) ) is that the hidden labels tend to be � sticky , � in that fields tend to consist of runs of the same label , as in figure 1 ( c ) , in contrast with part-of-speech tagging , where we rarely see adjacent tokens given the same label .
grenager et al. ( 2005 ) report that in order to learn this � sticky � structure , they had to alter the structure of their hmm so that a fixed mass is placed on each diagonal transition .
in this work , we learned this structure automatically though prototype similarity features without manually constraining the model ( see figure 8 ) , though we did change the similarity function ( see below ) .
on the test set of ( grenager et al. , 2005 ) , base scored an accuracy of 46.4 % , comparable to grenager et al. ( 2005 ) � s unsupervised hmm baseline .
adding the prototype list ( see figure 2 ) without distributional features yielded a slightly improved accuracy of 53.7 % .
for this domain , we utilized a slightly different notion of distributional similarity : we are not interested in the syntactic behavior of a word type , but its topical content .
therefore , when we collect context vectors for word types in this domain , we make no distinction by direction or distance and collect counts from a wider window .
this notion of distributional similarity is more similar to latent semantic indexing ( deerwester et al. , 1990 ) .
a natural consequence of this definition of distributional similarity is that many neighboring words will share the same prototypes .
therefore distributional prototype features will encourage labels to persist , naturally giving the � sticky � effect of the domain .
adding distributional similarity fea tures to our model ( proto + sim ) improves accuracy substantially , yielding 71.5 % , a 38.4 % error reduction over base.6 another feature of this domain that grenager et al. ( 2005 ) take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one .
grenager et al. ( 2005 ) experiment with manually adding boundary states and biasing transitions from these states to not self-loop .
we capture this � boundary � effect by simply adding a line to our protoype-list , adding a new boundary state ( see figure 2 ) with a few ( hand-chosen ) prototypes .
since we utilize a trigram tagger , we are able to naturally capture the effect that the boundary tokens typically indicate transitions between the fields before and after the boundary token .
as a post-processing step , when a token is tagged as a boundary token it is given the same label as the previous non-boundary token , which reflects the annotational convention that boundary tokens are given the same label as the field they terminate .
adding the boundary label yields significant improvements , as indicated by the proto + sim + bound setting in table 5.3 , surpassing the best unsupervised result of grenager et al. ( 2005 ) which is 72.4 % .
furthermore , our proto + sim + bound model comes close to the supervised hmm accuracy of 74.4 % reported in grenager et al. ( 2005 ) .
we also compared our method to the most basic semi-supervised setting , where fully labeled documents are provided along with unlabeled ones .
roughly 25 % of the data had to be labeled in order to achieve an accuracy equal to our proto + sim + bound model , suggesting that the use of prior knowledge in the prototype system is particularly efficient .
in table 5.3 , we provide the top confusions made by our proto + sim + bound model .
as can be seen , many of our confusions involve the feature field , which serves as a general purpose background state , which often differs subtly from other fields such as size .
for instance , the parenthical comment : ( master has walk - in closet with vanity ) is labeled as a size field in the data , but our model proposed it as a feature field .
neighborhood and address is another natural confusion resulting from the fact that the two fields share much of the same vocabulary ( e.g [ address 2525 telegraph ave . ] vs. [ nbrhd near telegraph ] ) .
conclusions .
we have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way .
these features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity .
another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings : the similarity guides the learning of the sequence model .
