abstract .
we describe the system that generated our submission for the 2006 clef question answering dutch monolingual task .
our system for this years task features entirely new question classification , data storage and access , and answer processing components .
introduction .
for our earlier participation in the clef question answering track ( 20032005 ) , we have developed a question answering architecture in which answers are generated by different competing strategies .
for the 2005 edition of clef-qa , we focused on converting our text resources to xml in order to make possible a qa-as-xml-retrieval strategy .
for 2006 , we have converted all of our data resources ( text , annotations , and tables ) to fit in an xml database in order to further standardize access .
additionally , we have devoted attention to improving known weak parts of our system : question classification , type checking , answer clustering , and score estimation .
this paper is divided in nine sections .
in section 2 , we give an overview of the current system architecture .
in the following four sections , we describe the changes we have made to our system for this year : question classification ( section 3 ) , storing data with multi-dimensional markup ( section 4 ) , and probabilistic answer processing ( sections 5 and 6 ) .
we present our submitted runs in section 7 and evaluate them in section 8 .
we conclude in section 9 .
system description .
the architecture of our quartz qa system is an expanded version of a standard qa architecture consisting of parts dealing with question analysis , information retrieval , answer extraction , and answer post-processing ( clustering , ranking , and selection ) .
the quartz architecture consists of multiple answer extraction modules , or streams , which share common question and answer processing components .
the answer extraction streams can be divided into three groups based on the text corpus that they employ : the clef-qa corpus , dutch wikipedia , or the web .
we describe these briefly here .
the common question and answer processing parts will be outlined in more detail in the next sections about the architecture updates .
the quartz system ( figure 1 ) contains four streams that generate answers from the clef-qa corpus .
the table lookup stream searches for answers in specialized knowledge bases which are extracted from the corpus offline ( prior to question time ) by predefined rules .
these rules take advantage of the fact that certain answer types , such as birthdays , are typically expressed in one of a small set of easily identifiable ways .
the ngrams stream looks for answers in the corpus by searching for word ngrams using a standard retrieval engine ( lucene ) .
the pattern match stream is quite similar except that it allows searching for regular expressions rather than just sequences of words .
the most advanced of the four clef-qa corpus streams is xquesta .
it performs xpath queries against an xml version of the clef-qa corpus which contains both the corpus text and additional annotations .
the annotations include information about part-of-speech , syntactic chunks , named entities , temporal expressions , and dependency parses ( from the alpino parser [ 7 ] ) .
xquesta retrieves passages of at least 400 characters , starting and ending at paragraph boundaries .
for 2006 , we have adapted several parts of our question answering system .
we changed the question classification process , the data storage method , and the answer processing module .
these topics are discussed in the following sections .
question classification .
an important problem identified in the error analysis of our previous clef-qa results was a mismatch between the type of the answer and the required type by the question .
a thorough evaluation of the type assignment and type checking parts of the system was required .
over the past three years , our question analysis moduleconsisting of regular expressions embedded in program codehad grown to a point that it was difficult to maintain .
therefore , we have re-implemented the module from scratch , aiming at classification by machine learning .
we started by collecting training data for the classifier .
the 600 questions from the three previous clef-qa tracks were an obvious place to start , but restricting ourselves to these questions was not an option , since our goal is to develop a general dutch qa system .
therefore , we added additional questions : 1000 questions from a dutch trivia game and 279 questions from other sourcesfrequently , questions provided by users from our online demo .
all the questions have been classified by a single annotator .
choosing a proper set of question classes was a non-trivial problem .
we needed question types not only for extracting candidate answers from text passages but also for selecting columns from tables .
sometimes , a coarse-grained type such as person would be sufficient , but for answering which-questions , a fine-grained type , such as american president , might be better .
we therefore decided on using three different types of question classes : a table type that linked the question to an available table column ( 17 classes ) , a coarse-grained type that linked the question to the types recognized by our named-entity recognizer ( 7 classes ) , and a fine-grained type that linked the question to wordnet synsets ( 166 classes ) .
for the machine learner , a question is represented as a collection of features .
we chose ten different features : in order to determine the top hypernym of the main noun of the question we use eurowordnet [ 8 ] .
we follow the hypernym path until we reach a word that a member of a predefined list which contains entities like persoon ( person ) or organisatie ( organisation ) .
the factoid / list flag value is also determined by the number value of the main noun .
a plural noun indicates a list question and a singular one , a factoid question .
questions without a noun will be classified by the number of the verb .
a question such as who is the new pope ? can be transformed to the features : who , are , pope , pope , pope , none , none , 0 , 0 and factoid .
we have not explored other features because with this small set of features we were already able to obtain a satisfactory score for coarse-grained classification ( about 90 % for the questions from previous clef-qa tracks ) .
we chose the memory-based learner timbl [ 4 ] for building the question classifier .
a known problem of this learner is that extra features can degrade the performance of the learner .
therefore , we performed an additional feature selection process in order to identify the best subset of features for the classification task ( using bidirectional hill-climbing , see [ 3 ] ) .
we performed three different experiments for determining the optimal feature sets for each of the three question classes .
each of the questions in the training set was classified by training on all the other questions ( leave-one-out ) .
the best score was obtained for the coarse-grained class using four features ( 4 , 5 , 8 and 9 ) : 85 % .
identifying fine-grained classes proved to be harder : 79 % , also with four features ( 4 , 7 , 8 and 9 ) .
table classes were best predicted with three features ( 80 % ; 4,9and 10 ) .
multi-dimensional markup .
our system makes use of several kinds of linguistic analysis tools , including a pos tagger , a named entity recognizer and a dependency parser [ 7 ] .
these tools are run offiine on the entire corpus , before any questions are posed .
the output of an analysis tool takes the form of a set of annotations : the tool identifies text regions in the corpus and associates some metadata with each region .
storing these annotations as xml seems natural and enables us to access them through the powerful xquery language .
ideally , we would like to query the combined annotations produced by several different tools at once .
this is not easily accomplished , though , because the tools may produce conflicting regions .
for example , a named entity may partially overlap with a phrasal constituent in such a way that the elements cannot be properly nested .
it is thus not possible , in general , to construct a single xml tree that contains annotations from all tools .
to deal with this problem , we have developed a general framework for the representation of multi-dimensional xml markup [ 2 ] .
this framework stores multiple layers of annotations referring to the same base document .
through an extension of the xquery language , it is possible to retrieve information from several layers with a single query .
this year , we migrated the corpus and all annotation layers to the new framework .
stand-off xml .
we store all annotations as stand-off xml .
textual content is stripped from the xml tree and stored separately in a blob file ( binary large object ) .
two region attributes are added to each xml element to specify the byte offsets of its start and end position with respect to the blob file .
this process can be repeated for all annotation layers , producing identical blobs but different stand-off xml markup .
the different markup layers are merged and stored as a single xml file .
although there will still be conflicting regions , this is no longer a problem because our approach does not require nested elements .
the added region attributes provide sufficient information to reconstruct each of the annotation layers , regardless of how they are merged .
in principle , we could merge the layers by simply concatenating the respective stand-off markup .
however , in order to simplify query formulation , we choose to properly nest the layers down to the level of sentence elements .
extending monetdb / xquery .
the merged xml documents are indexed using monetdb / xquery [ 1 ] , an xml database engine with full xquery support .
its xquery front-end consists of a compiler which transforms xquery programs into a relational query language internal to monetdb .
we extended the xquery language by defining four new path axes that allow us to step between layers .
the new axis steps relate elements by region overlap in the blob , rather than by nesting relations in the xml tree : select-narrow selects elements that have their region completely contained within the context elements region ; select-wide selects elements that have at least partial overlap with the context elements region ; reject-narrow and reject-wide select the non- contained and non-overlapping region elements , respectively .
the table in figure 2 demonstrates the results of each of the new axes when applied to our example document .
in addition to the new axes , we have also added an xquery function so-blob ( $ node ) .
this function takes an element and returns the contents of that elements blob region .
this function is necessary because all text content has been stripped from the xml markup , making it impossible to retrieve text directly from the xml tree .
the xquery extensions were implemented by modifying the front-end of monetdb / xquery .
an index on the offset attributes is used to make the new axis steps efficient even for large documents .
merging annotation layers .
we use a separate system , called xiraf , to coordinate the process of automatically annotating the corpus .
xiraf combines multiple text processing tools , each having an input descriptor and a tool-specific wrapper that converts the tool output into stand-off xml annotation .
the input descriptor associated with a tool is used to select regions in the data that are candidates for processing by that tool .
the descriptor may select regions on the basis of the original metadata or annotations added by other tools .
for example , our sentence splitter selects document text using the text element from original document markup .
other tools , such as the pos tagger and named-entity tagger , require separated sentences as input and thus use the output annotations of the sentence splitter by selecting sent elements .
some tools readily produce annotations in xml format , and other tools can usually be adapted to produce xml .
unfortunately , text processing tools may actually modify the input text in the course of adding annotations , which makes it non-trivial to associate the new annotations with regions in the original blob .
tools make a variety of modifications to their input text : some perform their own tokenization ( i.e. , inserting whitespaces or other word separators ) , silently skip parts of the input ( e.g. , syntactic parsers , when the parsing fails ) , or replace special symbols .
for many of the available text processing tools , such possible modifications are not fully documented .
xiraf , then , must re-align the output of the processing tools with the original blob .
we have experimented with a systematic approach that computes an alignment with minimimal edit- distance .
however , we found that there are special cases where a seriously incorrect alignment may have optimal edit-distance .
this prompted us to replace edit-distance alignment by ad-hoc alignment rules that are specific to the textual modifications made by each tool .
the alignment of blob regions is further complicated by the use of character encodings .
we use utf-8 encoding for blob files , enabling us to represent any sequence of unicode characters .
since utf-8 is a variable length encoding , this means that there is no direct relation between character offsets and byte offsets .
region attributes in our stand-off markup are stored as byte offsets to allow fast retrieval from large blob files .
many text processing tools , however , assume that their input is either ascii or latin-1 and produce character offsets in their output .
this makes it necessary for xiraf to carefully distinguish character offsets from byte offsets and convert between them in several situations .
after conversion and re-alignment , annotations are merged into the xml database .
in general , the output from a tool is attached to the element that produced the corresponding input .
for example , since the pos tagger requests a sentence as input , its output is attached to the sent element on which it was invoked .
in principle , our use of region attributes makes the tree structure of the xml database less important .
maintaining some logical structure in the tree , however , may allow for queries that are simpler or faster .
using multi-dimensional markup for qa .
two streams in our qa system have been adapted to work with multi-dimensional markup : the table lookup stream and xquesta .
the table stream relies on a set of tables that are extracted from the corpus offiine according to predefined rules .
these extraction rules were rewritten as xquery expressions and used to extract the tables from the collection text .
previous versions of xquesta had to generate separate xpath queries to retrieve annotations from several markup layers .
information from these layers had to be explicitly combined within the stream .
moving to multi-dimensional xquery enables us to query several annotation layers jointly , handing off the task of combining the layers to monetdb .
since xquery is a superset of xpath , previously developed query patterns could still be used in addition to the new , multi-dimentional ones .
probabilities .
one major consequence of the multi-stream architecture of the quartz qa system is the need for a module to choose among the candidate answers produced by the various streams .
the principal challenge of such a module is making sense of the confidence scores attached by each stream to its candidate answers .
this year , we made use of data from previous clef-qa campaigns in order to estimate correctness probabilities for candidate answers from stream confidence scores .
furthermore , we also estimated correctness probabilities conditioned on well-typedness in order to implement type-checking as bayesian update .
in the rest of this section , we describe how we estimated these probabilities .
we describe how we use these probabilities in the following section .
from scores to probabilities .
each stream of our qa system attaches confidence scores to the candidate answers it produces .
while these scores are intended to be comparable for answers produced by a single stream , there is no requirement that they be comparable across streams .
in order to make it possible for our answer re-ranking module ( described in section 6 ) to rank answers from different streams , we took advantage of answer patterns from previous editions of clef qa to estimate the probability that an answer from a given stream with a given confidence score is correct .
for each stream , we ran the stream over the questions from the previous editions of clef and binned the candidate answers by confidence score into 10 equally-sized bins .
then , for each bin , we used the available answer patterns to check the answers in the bin and based on these assessments , computed the maximum likelihood estimate for the probability that an answer with a score falling in the range of the bin would be correct .
with these probability estimates , we can now associate with a new candidate answer a correctness probability based on its confidence score .
type checking as bayesian update .
type checking can be seen as a way to increase the information we have about the possible correctness of an answer .
we discuss in section 6.3 how we actually type-check answers ; in this section , we explain how we incorporate the results of type-checking into our probabilistic framework .
one natural way to incorporate new information into a probabilistic framework is bayesian update .
given the prior probability of correctness for a candidate answer , p ( correct ) ( in our case , the mle corresponding with the stream confidence score ) , as well as the information that it is well- or ill-typed ( represented as the value of the random variable well typed ) , we compute p ( correct i well typed ) , the updated probability of correctness given well- ( or ill- ) typedness as follows .
in other words , the updated correctness probability of a candidate answer , given the information that it is well- or ill-typed , is the product of the prior probability and the ratio p ( well typed correct ) .
answer processing .
the multi-stream architecture of quartz embodies a high-recall approach to question answering the expectation is that using a variety of methods to find a large number of candidate answers should lead to a greater chance of finding correct answers .
the challenge , though , is choosing correct answers from the many candidate answers returned by the various streams .
the answer processing module described in this section is responsible for this task .
high-level overview .
the algorithm used by the quartz answer processing module is given here as algorithm 1 .
first , candidate answers for a given question are clustered ( line 2 : section 6.2 ) .
then , each cluster is evaluated in turn ( lines 39 ) .
each answer in the cluster is checked for well-formedness and well-typedness ( line 5 : section 6.3 ) : ans.well formed is a boolean value indicating whether some part of the original answer is well-formed ( ans.string is the corresponding part of the original answer ) ; ans.well typed is a boolean value indicating whether the answer is well-typed .
the correctness probability of the cluster p ( cluster ) ( line 7 ) is the combination of the updated correctness probabilities of the answers in the cluster ( the prior correctness probabilities p ( ans ) updated with type-checking information , as described in section 5.2 ) .
the longer of the well-formed answers in the cluster is then chosen as the representative answer for the cluster ( line 8 ) .
finally , the clusters are sorted according to their correctness probabilities ( line 10 ) .
answer clustering .
answer candidates with similar or identical answer strings are merged into clusters .
in previous versions of our system , this was done by repeatedly merging pairs of similar answers until no more similar pairs could be found .
after each merge , the longest of the two answer strings was selected and used for further similarity computations .
this year , we moved to a graph-based clustering method .
formulating answer merging as a graph clustering problem has the advantage that it better captures the non-transitive nature of answer similarity .
for example , it may be the case that the answers oorlog and wereldoorlog should be considered similar , as well as wereldoorlog and wereldbeeld , but not oorlog and wereldbeeld .
to determine which of these answers should be clustered together , it may be necessary to take into account similarity relations with the rest of the answers .
our clustering method operates on a matrix that contains a similarity score for each pair of answers .
the similarity score is an inverse exponential function of the edit distance between the strings , normalized by the sum of the string lengths .
the number of clusters is not set in advance but is determined by the algorithm .
we used an existing implementation of a spectral clustering algorithm [ 5 ] to compute clusters within the similarity graph .
the algorithm starts by putting all answers in a single cluster , then recursively splits clusters according to spectral analysis of the similarity matrix .
splitting stops when any further split would produce a pair of clusters for which the normalized similarity degree exceeds a certain threshold .
the granularity of the clusters can be controlled by changing the threshold value and the parameters of the similarity function .
checking individual answers .
the typing and well-formedness checks performed on answers depends primarily on the expected answer type of the question .
real type-checking can only be performed on questions whose expected answer type is a named-entity , a date , or a numeric expression .
for other questions , only basic well-formedness checks are performed .
note that the probability update ratios described in section 5.2 are only computed on the basis of answers that are type-checkable .
for answers to other questions , we use the same ratio for ill-formed answers as we do for ill-typed answers ( 0.34 ) , but we do not compute any update for well-formed answers ( i.e. , we update with a ratio of 1.0 ) .
for answers to all questions , two basic checks are performed .
if an answer consists solely of non-alphanumeric characters or is wholly contained as a substring of the question , it is immediately rejected as ill-formed and ill-typed .
for answers to questions with the following answer types only the very basic well-formedness checks noted are additionally performed : for questions expecting named entities , dates , or numeric answers , more significant wellformedness and well-typedness checks are performed .
for numeric answers , we check that the answer consists of a number ( either in digits or spelled out ) with optional modifiers ( e.g. , ongeveer , meer dan , etc . ) and units .
for names and dates , we look at the justification snippet and run an ne-tagger or date tagger on the snippet , as appropriate .
in addition to verifying that the candidate answer is a name ( for well-formedness ) of the correct type ( for well-typedness ) , we also check whether there are additional non-name words at the edges of the answer and remove them , if necessary .
the results of answer checking are then boolean values for well-formedness and well-typedness , as well as the possibly edited answer string .
we submitted two dutch monolingual runs .
the run uams06tnlnl used the full system with all streams and final answer selection .
the run uams06nnlnl used the full system but without type-checking .
results .
the question classifier performed as expected for coarse question classes : 86 % correct compared with a score of 85 % on the training data .
for most of the classes , precision and recall scores were higher than 80 % ; the exceptions are miscellaneous and number .
for assigning table classes , the score was much lower than for the training data : 56 % compared with 80 % .
we did not evaluate fine-grained class assignment because these classes are not used in the current version of the system .
table 1 lists the assessment counts for the two university of amsterdam runs for the question answering track of clef-2006 .
the two runs had 14 different top answers of which four were assessed differently .
in 2005 our two runs contained a large number of inexact answers ( 28 and 29 ) .
we are happy about those numbers being lower in 2006 ( 4 and 4 ) and the most frequent problem , extraneous information added to a correct answer , has almost disappeared .
however , the number of correct answers dropped as well , from 88 in 2005 to 41 .
this is at least partly caused by the fact that the questions were more difficult this year .
like last year , the questions could be divided in two broad categories : questions asking for lists and questions requiring singular answers .
the second category can be divided in three subcategories : questions asking for factoids , questions asking for definitions and temporally restricted questions .
we have examined the uams06tnlnl-run answers to the questions of the different categories in more detail ( table 2 ) .
our system failed to generate any correct answers for the list questions .
for the non-list questions , 21 % of the top answers were correct .
within this group , the temporally restricted questions ' ( 8 % correct ) posed the biggest challenge to our system .
in 2005 , we saw similar differences between factoid , definition and temporally restricted questions .
at that time the difference between the last category and the first two could be explained by the presence of a significant number of incorrect answers which would have been correct in another time period .
this year , no such answers were produced by our system .
more than 75 % of our answers were incorrect .
we examined the answers given to the first twelve questions in more detail in order to find out what the most important problems were ( see appendix a ) .
the top answer to the first question is leak in the tank of space shuttle .
it is unclear to us why the correct and more frequent apposition space shuttle was not selected as the answer .
for question two , three of the top five answers are correct , but the top answer is wrong .
it consists of a long clause which seems to have been identified as an apposition .
the third question has been answered correctly .
all top five answers are short .
no correct answer was found for question four because the date in the question was wrong ( should have been 1938 ) .
question five was answered correctly ( nil ) .
for question six , the top answer is inexact because it only mentions a city and not the name of the concert hall .
the other four answers are poorly motivated by the associated snippets .
the same is true for all the answers generated for question seven .
here , the correct answer is nil .
four of the five answers produced for question eight do not have the correct type .
they should have been a time period , but apart from the top answer , they consist of arbitrary numeric expressions .
question nine is hard to answer .
we have not been able to find the required answer ( one million ) in the collection .
our system generated five numbers which had nothing to do with the required topic .
two of the numbers were clearly wrong : 000 .
the top answer for question ten has an incorrect type as well ( noun rather than the required organization ) .
no answers were generated for questions eleven and twelve .
the first required a search term expansion ( from un to united nations ) .
the nil answer for the second is a surprise .
the question phrases foreign , movie and oscar can be found back in the collection close the correct answers which have the required type .
based on this analysis , we have created the following list of future work : as the answers to the first two questions show , our system still prefers infrequent long answers over frequent short ones .
this often leads to highly ranked incorrect answers .
we should change the ranking part of the system and give a higher weight to frequency rather than string length .
another way to decrease the number of incorrect long answers is to prevent them from appearing in the first place .
this means changing our information extraction modules .
one part of our architecture which is currently poorly understood is the part that generates queries .
in our analysis , we found several cases of undergeneration and overgeneration of answers .
we suspect that the query generation component is involved in the generation of at least some of these answers .
although part of our work has been focused on matching types of answers with question types , it seems that our current system still has problems in this area ( questions nine and ten ) .
more work is required here .
in the last four years , we have submitted multiple runs with minor differences .
given our current scoring level it would be interesting to aim at runs that are more different .
this could have a positive effect on our standing in the evaluation .
' the official assessments mention the presence of only one temporally restricted question , which is very unlikely .
we have regarded all questions with time expressions ( 32 ) as temporally restricted .
conclusion .
we have described the fourth iteration of our system for the clef question answering dutch mono-lingual track ( 2006 ) .
this year , our work has focused on converting all data repositories of the system ( text , annotation and tables ) to xml and allowing them to be accessed via the same interface .
additionally , we have modified parts of our system which we had suspected of weaknesses : question classification and answer processing .
at this point , we would like to know if the modifications of the system have resulted in improvemed performance .
the systems performance on this years questions ( 20 % correct ) was much worse than in 2005 ( 45 % ) .
however , the 2006 questions were more difficult than those of last year , and we expect that the workshop will show that the performance of other participants has also dropped .
it would be nice to be able to run last years system on the 2006 questions but unfortunately , we do not have a copy of that system available .
applying the current system to last years questions is something we can and should do , but the outcome of that experiment may not be completely reliable since those questions have been used for tuning the system .
at this moment , we simply do not know if our work has resulted in a better system .
one thing we know for certain : there is a lot of room for improvement .
obtaining 20 % accuracy on factoid questions is still far away from the 70 % scores obtained by the best systems participating in trec .
luckily , our 2006 clef-qa participation has identified key topics for future work : information extraction , query generation , answer type checking , and answer ranking .
we hope that work on these topics will lead to better performance in 2007 .
