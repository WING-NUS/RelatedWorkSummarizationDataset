(claim)#the goal of text classification is to classify the topic or theme of a document $1 .
0#(23)#automated text classification is a supervised learning task , defined as automatically assigning pre-defined category labels to documents $1 .
(claim)#it is a well studied task , with many effective techniques .
(claim)#feature selection is known to be important .
(claim)#the purpose of feature selection is to reduce the dimensionality of the term space since high dimensionality may result in the overfitting of a classifier to the training data .
0.1#(24)#yang and pedersen studied five feature selection methods for aggressive dimensionality reduction : term selection based on document frequency ( df ) , information gain ( ig ) , mutual information , a ? 2 test ( ciii ) , and term strength $1 . using the knn and linear least squares fit mapping ( llsf ) techniques , they found ig and ciii most effective in aggressive term removal without losing categorization accuracy . they also found that df thresholding , the simplest method with the lowest cost in computation could reliably replace ig or ciii when the computations of those measure were expensive .
0.2#(18)#popular techniques for text classification include probabilistic classifiers ( e.g , naive bayes classifiers ) , decision tree classifiers , regression methods ( e.g. , linear least-square fit ) , on-line ( filtering ) methods ( e.g. , perceptron ) , the rocchio method , neural networks , example-based classifiers ( e.g. , knn ) , support vector machines , bayesian inference networks , genetic algorithms , and maximum entropymodelling $1 .
0.2#(23)#yang and liu $1 conducted a controlled study of 5 well-known text classification methods : support vector machine ( svm ) , k-nearest neighbor ( knn ) , a neural network ( nnet ) , linear least-square fit ( llsf ) mapping , and naive bayes ( nb ) . their results show that svm , knn , and llsf significantly outperform nnet and nb when the number of positive training examples per category are small ( fewer than 10 ) .
(claim)#in monolingual text classification , both training and test data are in the same language . there have been only a few studies on this issue .
0.4#(21)#in 1999 , topic detection and tracking ( tdt ) research was extended from english to chinese $1 .
0.4#(4)#in topic tracking , a system is given several ( e.g. , 1-4 ) initial seed documents and asked to monitor the incoming news stream for further documents on the same topic $1 , the effectiveness of cross-language classifiers ( trained on chinese data and tested on english ) was worse than monolingual classifiers .
(claim)#cross-language text classification emerges when training data are in some other language .
0.4#(2)#bel et al. $1 studied an english-spanish bilingual classification task for the international labor organization ( ilo ) corpus , which had 12 categories . they tried two approaches a poly-lingual approach in which both english and spanish training and test data were available , and cross-lingual approach in which training examples were available in one language . using the poly-lingual approach , in which a single classifier was built from a set of training documents in both languages , their winnow classifier , which , like svm , computes an optimal linear separator in the term space between positive and negative training examples , achieved f1 of 0.811 , worse than their monolingual english classifier ( with f1 = 0.865 ) but better than their monolingual spanish classifier ( with f1 = 0.790 ) . for the cross-lingual approach , they used two translation methods terminology translation and profile translation .
(claim)#when trained on english and tested on spanish translated into english , their classifier achieved f1 of 0.792 using terminology translation and 0.724 using profile translation ; when trained on spanish and tested on pseudo-spanish , their classifier achieved f1 of 0.618 ; all worse than their corresponding monolingual classifiers .
0.4#(17)#rigutini et al. $1 studied english and italian cross-language text classification in which training data were available in english and the documents to be classified were in italian . they used a naive bayes classifier to classify english and italian newsgroups messages of three categories : hardware , auto and sports . english training data ( 1,000 messages for each category ) were translated into italian using office translator idiomax . their cross-language classifier was created using expectation maximation ( em ) , with english training data ( translated into italian ) used to initialize the em iteration on the unlabeled italian documents . once the italian documents were labeled , these documents were used to train an italian classifier . the cross-language classifier performed slightly worse than monolingual classifier , probably due to the quality of their translated italian data .
0.4#(5)#$1 investigated english and italian cross-language text classification by using comparable corpora and bilingual dictionaries ( multiwordnet and the collins english-italian bilingual dictionary ) . the comparable corpus was used for latent semantic analysis which exploits the presence of common words among different languages in the term-by-document matrix to create a space in which documents in both languages were represented . their cross-language classifier , either trained on english and tested on italian , or trained on italian and tested on english , achieved an f1 of 0.88 , worse than their monolingual classifier ( with f1 = 0.95 for english and 0.92 for italian ) .
0.4#(16)#$1 classified czech documents using english training data . they translated czech document vectors into english document vectors using a probabilistic dictionary which contained conditional word-translation probabilities for 46,150 word translation pairs . their concept label knn classifier ( k = 20 ) achieved precision of 0.40 , which is 73 % of the precision of a corresponding monolingual classifier .
(claim)#the main differences of our approach compared with earlier approaches include : ( 1 ) classifying document segments into aspects , rather than documents into topics ; ( 2 ) using few training examples from both languages ; ( 3 ) using statistical machine translation results to map segment vectors from one language into the other .
