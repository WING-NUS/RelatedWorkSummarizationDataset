hidden topic markov models .
abstract .
algorithms such as latent dirichlet allocation ( lda ) have achieved significant progress in modeling word document relationships .
these algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document .
given these parameters , the topics of all words in the same document are assumed to be independent .
in this paper , we propose modeling the topics of words in the document as a markov chain .
specifically , we assume that all words in the same sentence have the same topic , and successive sentences are more likely to have the same topics .
since the topics are hidden , this leads to using the well-known tools of hidden markov models for learning and inference .
we show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics .
quantitatively , we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity .
introduction .
extensively large text corpora are becoming abundant due to the fast growing storage and processing capabilities of modern computers .
this has lead to a resurgence of interest in automated extraction of useful information from text .
modeling the observed text as generated from latent aspects or topics is a prominent approach in machine learning studies of texts ( e.g. , [ 8 , 14 , 3 , 4 , 6 ] ) .
in such models , the bag-of-words assumption is often employed , an assumption that the order of words can be ignored and text corpora can be represented by a co-occurrence matrix of words and documents .
the probabilistic latent semantic analysis ( plsa ) model [ 8 ] is such a model .
it employs two parameters inferred from the observed words : ( a ) a global parameter that ties the documents in the corpora , is fixed for the corpora and stands for the probability of words given topics .
( b ) a set of parameters for each of the documents that stand for the probability of topics in a document .
the latent dirichlet allocation ( lda ) model [ 3 ] introduces a more consistent probabilistic approach as it ties the parameters of all documents via a hierarchical generative model .
the mixture of topics per document in the lda model is generated from a dirichlet prior mutual to all documents in the corpus .
these models are not only computationally efficient , but also seem to capture correlations between words via the topics .
yet , the assumption that the order of words can be ignored is an unrealistic oversimplification .
relaxing this assumption is expected to yield better models in terms of the latent aspects inferred , their performance in word sense disambiguation task and the ability of the model to predict previously unobserved words in trained documents .
markov models such as n-grams and hmms that capture local dependencies between words have been employed mainly in part-of-speech tagging [ 5 ] .
models for semantic parsing tasks often use a shallow model with no hidden states [ 9 ] .
in recent years several probabilistic models for text that infer topics and incorporate markovian relations have been studied .
in [ 7 ] a model that integrates topics and syntax is introduced .
it contains a latent variable per each word that stands for syntactic classes .
the model posits that words are either generated from topics that are randomly drawn from the topic mixture of the document or from the syntactic classes that are drawn from the previous syntactic class .
only the latent variables of the syntactic classes are treated as a sequence with local dependencies while latent assignments of topics are treated similar to the lda model , namely topics extraction is not benefited from the additional information conveyed in the structure of words .
during the last couple of years , a few models were introduced in which consecutive words are modeled by markovian relations .
these are the bigram topic model [ 15 ] , the lda collocation model and the topical n-grams model [ 16 ] .
all these models assume that words generation in texts depend on a latent topic assignment as well as on the n-previous words in the text .
this added complexity seem to provide the models with more predictive power compared to the lda model and to capture relations between consecutive words .
we follow the same lines , while we allow markovian relations between the hidden aspects .
a somewhat related model is the aspect hmm model [ 2 ] , though it models unstructured data that contains stream of words .
the model contains latent topics that have markovian relations .
in the aspect hmm model , documents or segments are inferred using heuristics that assume that each segment is generated from a unique topic assignment and thus there is no notion of mixture of topics .
we strive to extract latent aspects from documents by making use of the information conveyed in the division into documents as well as the particular order of words in each document .
following these lines we propose in this paper a novel and consistent probabilistic model we call the hidden topic markov model ( htmm ) .
our model is similar to the lda model in tying together parameters of different documents via a hierarchical generative model , but unlike the lda model it does not assume documents are bags of words .
rather it assumes that the topics of words in a document form a markov chain , and that subsequent words are more likely to have the same topic .
we show that the htmm model outperforms the lda model in its predictive performance and can be used for text parsing and word sense disambiguation purposes .
hidden topic markov model for text documents .
we start by reviewing the formalism of lda .
figure 1a shows the graphical model corresponding to the lda generative model .
to generate a new word w in a document , one starts by first sampling a hidden topic z from a multinomial distribution defined by a vector ^ corresponding to that document .
given the topic z , the distribution over words is multinomial with parameters ^ , .
the lda model ties parameters between different documents by drawing ^ of all documents from a common dirichlet prior parameterized by ^ .
it also ties parameters between topics by drawing the vector ^ , of all topics from a common dirichlet prior parameterized by ^ .
in order to make the independence assumptions in lda more explicit , figure 1b shows an alternative graphical model for the same generative process .
here , we have explicitly denoted the hidden topics zi and the observed words wi as separate nodes in the graphs ( rather than summarizing them with a plate ) .
from figure 1b it is evident that conditioned on ^ and ^ , the hidden topics are independent .
the hidden topic markov model drops this independence assumption .
as shown in figure 1c , the topics in a document form a markov chain with a transition probability that depends on ^ and a topic transition variable ^ n .
when ^ n = 1 , a new topic is drawn from ^ .
when ^ n = 0 the topic of the nth word is identical to the previous one .
we assume that topic transitions can only occur between sentences , so that ^ n may only be nonzero for the first word in a sentence .
unlike the lda and mixture of unigrams models , the htmm model ( which allows infrequent topic transitions within a document ) is no longer invariant to a reshuffling of the words .
documents for which successive words have the same topics are more likely than a random permutation of the same words .
the input to the algorithm is the entire document , rather than a document-word co-occurrence matrix .
this obviously increases the storage requirement for each document , model proposed in this paper .
the hidden topics form a markov chain .
the order of words and their proximity plays an important role in the model. but it allows us to perform inferences that are simply impossible in the bag of words models .
for example , the topic assignments calculated during htmm inference ( either hard assignments or soft ones ) will not give the same topic to all appearances of the same word within the same document .
this can be useful for word sense disambiguation in applications such as automatic translation of texts .
furthermore , the topic assignment will tend to be linearly coherent - subsections of the text will tend to be assigned to the same topic , as opposed to bag-of-words assignments which may fluctuate between topics in successive words .
approximate inference .
due to the parameter-tying in lda type models , calculating exact posterior probabilities over the parameters ^ , ^ is intractable .
in recent years , several alternatives for approximate inference have been suggested : em [ 8 ] or variational em [ 3 ] , expectation propagation ( ep ) [ 10 ] and monte-carlo sampling [ 13 , 7 ] .
in this paper , we take advantage of the fact that conditioned on ^ and ^ the htmm model is a special type of hmm .
this allows us to use the standard parameter estimation tools of hmms , namely expectation-maximization and the forward-backward algorithm [ 12 ] .
unlike fully bayesian inference methods , the standard em algorithm for hmms distinguishes between latent variables and parameters .
applied to the htmm model , the latent variables are the topics zn and the variables ^ n that determine whether the topic n will be identical to topic n 1 or will it be drawn according to ^ d .
the parameters of the problem are ^ d , ^ and ^ .
we assume the hyper parameters ^ and ^ to be known. each sentence in the document .
we compute this probability using the forward-backward algorithm for hmm .
the transition matrix is specific per document and depends on the parameters ^ d and ^ .
the parameter ^ z , w induces local probabilities on the sentences .
after computing pr ( zn , ^ nld , w1 ... wnd ) we compute expectations required for the m-step .
specifically , we compute ( 1 ) the expected number of topic transitions that ended in the topic z and ( 2 ) the expected number of co-occurrence of a word w with a topic z .
em algorithms have been discussed for word document models in several recent papers .
hofmann used em to estimate the maximum likelihood parameters in the plsa model [ 8 ] .
here , in contrast , we use em to estimate the map parameters in a hierarchical generative model similar to lda ( note that our m step explicitly takes into account the dirichlet priors on ^ , ^ ) .
griffiths et al. [ 6 ] also discuss using em in their hmm model of topics and syntax and say that gibbs sampling is preferable since em can suffer from local minima .
we should note that in our experiments we found the final solution calculated by em to be stable with respect to multiple initializations .
again , this may be due to our use of a hierarchical generative model which reduces somewhat the degrees of freedom available to em .
our em algorithm assumes the hyper parameters ^ , ^ are fixed .
we set these parameters according to the values used in previous papers [ 13 ] ( ^ = 1 + 50 / k and ^ = 1.01 ) .
experiments .
we first demonstrate our results on the nips dataset the nips dataset consists of 1740 documents .
the train set consists of 1557 documents and the test set consists of the remaining 183 .
the vocabulary contains 12113 words .
from the raw data we extracted ( only ) the words that appear in the vocabulary , preserving their order .
stop words do not appear in the vocabulary , hence they were discarded from the input .
we divided the text to sentences according to the delimiters . ? ! ; .
this simple preprocessing is very crude and noisy since abbreviations such as fig . 1 will terminate the sentence before the period and will start a new one after it .
the only exception is that we omitted appearances of e.g. and i.e. in our preprocessing sentences are drawn from ^ independently .
in all the experiments the same values of ^ , ^ were provided to all algorithms .
figure 2 shows the perplexity of htmm , lda and vhtmm1. with k = 100 topics as a function of the number of observed words at the beginning of the test document , n. we see that for a small number of words , both htmm and vhtmm1 have significantly lower perplexity than lda .
this is due to combining the information from subsequent words regarding the topic of the sentence .
we give necessary and sufficient conditions for uniqueness of the support vector solution for the problems of pattern recognition and regression estimation , for a general class of cost functions .
we show that if the solution is not unique , all support vectors are necessarily at bound , and we give some simple examples of non-unique solu- tions .
we note that uniqueness of the primal ( dual ) solution does not necessarily imply uniqueness of the dual ( primal ) solution .
we show how to compute the threshold b when the solution is unique , but when all support vectors are at bound , in which case the usual method for determining b does not work .
