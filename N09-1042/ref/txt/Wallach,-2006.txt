topic modeling : beyond bag-of-words .
abstract .
some models of textual corpora employ text generation methods involving n-gram statistics , while others use latent topic variables inferred using the bag-of-words assumption , in which word order is ignored .
previously , these methods have not been combined .
in this work , i explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical dirichlet bigram language model .
the model hyperparameters are inferred using a gibbs em algorithm .
on two data sets , each of 150 documents , the new model exhibits better predictive accuracy than either a hierarchical dirichlet bigram language model or a unigram topic model .
additionally , the inferred topics are less dominated by function words than are topics discovered using unigram statistics , potentially making them more meaningful .
introduction .
recently , much attention has been given to generative probabilistic models of textual corpora , designed to identify representations of the data that reduce description length and reveal inter- or intra- document statistical structure .
such models typically fall into one of two categoriesthose that generate each word on the basis of some number of preceding words or word classes and those that generate words based on latent topic variables inferred from word correlations independent of the order in which the words appear .
n-gram language models make predictions using observed marginal and conditional word frequencies .
while such models may use conditioning contexts of arbitrary length , this paper deals only with bigram modelsi.e. , models that predict each word based on the immediately preceding word .
to develop a bigram language model , marginal and conditional word counts are determined from a corpus w .
the marginal count ni is defined as the number of times that word i has occurred in the corpus , while the conditional count nij is the number of times word i immediately follows word j .
given these counts , the aim of bigram language modeling is to develop predictions of word wt given word wt-1 , in any document .
typically this is done by computing estimators of both the marginal probability of word i and the conditional probability of word i following word j , such as fi = ni / n and fijj = nijj / nj , where n is the number of words in the corpus .
if there were sufficient data available , the observed conditional frequency fij could be used as an estimator for the predictive probability of i given j .
in practice , this does not provide a good estimate : only a small fraction of possible i , j word pairs will have been observed in the corpus .
consequently , the conditional frequency estimator has too large a variance to be used by itself .
the parameter a may be fixed , or determined from the data using techniques such as cross-validation ( jelinek & mercer , 1980 ) .
this procedure works well in practice , despite its somewhat ad hoc nature .
the hierarchical dirichlet language model ( mackay & peto , 1995 ) is a bigram model that is entirely driven by principles of bayesian inference .
this model has a similar predictive distribution to models based on equation ( 1 ) , with one key difference : the bigram statistics fijj in mackay and petos model are not smoothed with marginal statistics fi , but are smoothed with a quantity related to the number of different contexts in which each word has occurred .
topic modeling : beyond bag-of-words .
latent dirichlet allocation ( blei et al. , 2003 ) provides an alternative approach to modeling textual corpora .
documents are modeled as finite mixtures over an underlying set of latent topics inferred from correlations between words , independent of word order .
this bag-of-words assumption makes sense from a point of view of computational efficiency , but is unrealistic .
in many language modeling applications , such as text compression , speech recognition , and predictive text entry , word order is extremely important .
furthermore , it is likely that word order can assist in topic inference .
the phrases the department chair couches offers and the chair department offers couches have the same unigram statistics , but are about quite different topics .
when deciding which topic generated the word chair in the first sentence , knowing that it was immediately preceded by the word department makes it much more likely to have been generated by a topic that assigns high probability to words related to university administration .
in practice , the topics inferred using latent dirichlet allocation are heavily dominated by function words , such as in , that , of and for , unless these words are removed from corpora prior to topic inference .
while removing these may be appropriate for tasks where word order does not play a significant role , such as information retrieval , it is not appropriate for many language modeling applications , where both function and content words must be accurately predicted .
in this paper , i present a hierarchical bayesian model that integrates bigram-based and topic-based approaches to document modeling .
this model moves beyond the bag-of-words assumption found in latent dirichlet allocation by introducing properties of mackay and petos hierarchical dirichlet language model .
in addition to exhibiting better predictive performance than either mackay and petos language model or latent dirichlet allocation , the topics inferred using the new model are typically less dominated by function words than are topics inferred from the same corpora using latent dirichlet allocation .
background .
hierarchical dirichlet language model .
the hyperparameter mi is now taking the role of the marginal statistic fi in equation ( 1 ) .
additionally , the prior over / gym may be assumed to be uninformative , yielding a minimal data- driven bayesian model in which the optimal / gym may be determined from the data by maximizing the evidence .
mackay and peto show that each element of the optimal m , when estimated using this empirical bayes procedure , is related to the number of contexts in which the corresponding word has appeared .
latent dirichlet allocation .
latent dirichlet allocation ( blei et al. , 2003 ) represents documents as random mixtures over latent topics , where each topic is characterized by a distribution over words .
each word wt in a corpus w is assumed to have been generated by a latent topic zt , drawn from a document-specific distribution over t topics .
word generation is defined by a conditional distribution p ( wt = ilzt = k ) , described by t ( w 1 ) free parameters , where t is the number of topics and w is the size of the vocabulary .
these parameters are denoted by 4 ) , with p ( wt = ilzt = k ) ^ ilk . 4 ) may be thought of as an emission probability matrix , in which the kth row , the distribution over words for topic k , is denoted by ok .
similarly , topic generation is characterized by a conditional distribution p ( zt = kldt = d ) , described by d ( t 1 ) free parameters , where d is the number of documents in the corpus .
these parameters form a matrix o , with p ( zt = kldt = d ) bkld .
the dth row of this matrix is the distribution over topics for document d , denoted by ed .
the sum over z cannot be computed directly because it does not factorize and involves tn terms , where n is the total number of words in the corpus .
however , it may be approximated using markov chain monte carlo ( griffiths & steyvers , 2004 ) .
bigram topic model .
this section introduces a model that extends latent dirichlet allocation by incorporating a notion of word order , similar to that employed by mackay and petos hierarchical dirichlet language model .
each topic is now represented by a set of w distributions .
however , the additional conditioning context j in the distribution that defines word generation affords greater flexibility in choosing a hierarchical prior for 4 ) than in either latent dirichlet allocation or the hierarchical dirichlet language model .
the priors over 4 ) used in both mackay and petos language model and blei et al.s latent dirichlet allocation are coupled priors : learning the probability vector for a single context , oj the case of mackay and petos model and ok in blei et al.s , gives information about the probability vectors in other contexts , j ' and k ' respectively .
this dependence comes from the hyperparameter vector om , shared , in the case of the hierarchical dirichlet language model , between all possible previous word contexts j and , in the case of latent dirichlet allocation , between all possible topics k .
since word generation is conditioned upon both j and k in the new model presented in this paper , there is more than one way in which hyperparameters for the prior over 4 ) might be shared in this model .
information is now shared between only those probability vectors with topic context k .
intuitively , this is appealing .
learning about the distribution over words for a single context j , k yields information about the distributions over words for other contexts j ' , k that share this topic , but not about distributions with other topic contexts .
in other words , this prior encapsulates the notion of similarity between distributions over words for a given topic context .
as in latent dirichlet allocation , the sum over z is intractable , but may be approximated using mcmc .
inference of hyperparameters .
previous sampling-based treatments of latent dirichlet allocation have not included any method for optimizing hyperparameters .
however , the method described in this section may be applied to both latent dirichlet allocation and the model presented in this paper .
the evidence contains latent variables z and must therefore be maximized with respect to the hyperparameters using an expectation-maximization ( em ) algorithm .
unfortunately , the expectation with respect to the distribution over the latent variables involves a sum over tn terms , where n is the number of words in the entire corpus .
however , this sum may be approximated using a markov chain monte carlo algorithm , such as gibbs sampling , resulting in a gibbs em algorithm ( andrieu et al. , 2003 ) .
gibbs sampling involves sequentially sampling each variable of interest , zt here , from the distribution over that variable given the current values of all other variables and the data .
letting the subscript t denote a quantity that excludes data from the tth position , the conditional posterior for zt is either ( prior 1 ) .
drawing a single set of topics z takes time proportional to the size of the corpus n and the number of topics t. the e-step therefore takes time proportional to n , t and the number of iterations for which the markov chain is run in order to obtain the s samples .
note that the samples used to approximate the e-step must come from a single markov chain .
the model is unaffected by permutations of topic indices .
consequently , there is no correspondence between topic indices across samples from different markov chains : topics that have index k in two different markov chains need not have similar distributions over words .
m-step .
in my implementation , each fixed-point iteration takes time that is proportional to s and ( at worst ) n. for latent dirichlet allocation and the new model with prior 1 , the time taken to perform the m-step is therefore at worst proportional to s , n and the number of iterations taken to reach convergence .
for the new model with prior 2 , the time taken is also proportional to t. experiments .
to evaluate the new model , both variants were compared with latent dirichlet allocation and mackay and petos hierarchical dirichlet language model .
the topic models were trained identically : the gibbs em algorithm described in the previous section was used for both the new model ( with either prior ) and latent dirichlet allocation .
the hyperparameters of the hierarchical dirichlet language model were inferred using the same fixed-point iteration used in the m-step .
the results presented in this section are therefore a direct reflection of differences between the models .
language models are typically evaluated by computing the information rate of unseen test data , measured in bits per word : the better the predictive performance , the fewer the bits per word .
information rate is a direct measure of text compressibility .
corpora .
the models were compared using two data sets .
the first was constructed by drawing 150 abstracts ( documents ) at random from the psychological review abstracts data provided by griffiths and steyvers ( 2005 ) .
a subset of 100 documents were used to infer the hyperparameters , while the remaining 50 were used for evaluating the models .
the second data set consisted of 150 newsgroup postings , drawn at random from the 20 newsgroups data ( rennie , 2005 ) .
again , 100 documents were used for inference , while 50 were retained for evaluating predictive accuracy .
punctuation characters , including hyphens and apostrophes , were treated as word separators , and each number was replaced with a special number token to reduce the size of the vocabulary .
to enable evaluation using documents containing tokens not present in the training corpus , all words that occurred only once in the training corpus were replaced with an unseen token u .
preprocessing the psychological review abstracts data in this manner resulted in a vocabulary of 1374 words , which occurred 13414 times in the training corpus and 6521 times in the documents used for testing .
the 20 newsgroups data ended up with a vocabulary of 2281 words , which occurred 27478 times in the training data and 13579 times in the test data .
despite consisting of the same number of documents , the 20 newsgroups corpora are roughly twice the size of the psychological review abstracts corpora .
results .
the experiments involving latent dirichlet allocation and the new model were run with 1 to 120 topics , on an opteron 254 ( 2.8ghz ) .
these models all required at most 200 iterations of the gibbs em algorithm described in section 4 .
in the e-step , a markov chain was run for 400 iterations .
the first 200 iterations were discarded and 5 samples were taken from the remaining iterations .
the mean time taken for each iteration is shown for both variants of the new model as a function of the number of topics in figure 2 .
as expected , the time taken is proportional to both the number of topics and the size of the corpus .
the information rates of the test data are shown in figure 1 .
on both corpora , latent dirichlet allocation and the hierarchical dirichlet language model achieve similar performance .
with prior 1 , the new model improves upon this by between 0.5 and 1 bits per word .
however , with prior 2 , it achieves an information rate reduction of between 1 and 2 bits per word .
for latent dirichlet allocation , the information rate is reduced most by the first 20 topics .
the new model uses a larger number of topics and exhibits a greater information rate reduction as more topics are added .
in latent dirichlet allocation , the latent topic for a given word is inferred using the identity of the word , the number of times the word has previously been assumed to be generated by each topic , and the number of times each topic has been used in the current document .
in the new model , the previous word is also taken into account .
this additional information means words that were considered to be generated by the same topic in latent dirichlet allocation , may now be assumed to have been generated by different topics , depending on the contexts in which they are seen .
consequently , the new model tends to use a greater number of topics .
in addition to comparing predictive accuracy , it is instructive to look at the inferred topics .
table 1 shows the words most frequently assigned to a selection of topics extracted from the 20 newsgroups training data by each of the models .
the unseen token was omitted .
the topics inferred using latent dirichlet allocation contain many function words , such as the , in and to .
in contrast , all but one of the topics inferred by the new model , especially with prior 2 , typically contain fewer function words .
instead , these are largely collected into the single remaining topic , shown in the last column of rows 2 and 3 in table 1 .
this effect is similar , though less pronounced , to that achieved by griffiths et al.s composite model ( 2004 ) , in which function words are handled by a hidden markov model , while content words are handled by latent dirichlet allocation .
future work .
here , information is shared between all distributions with previous word context j .
this prior captures the notion of common bigramsword pairs that always occur together .
however , the number of hyperparameter vectors is extremely largemuch larger than the number of hyperparameters in prior 2with comparatively little data from which to infer them .
to make effective use of this prior , each normalized measure mj should itself be assigned a dirichlet prior .
this variant of the model could be compared with those presented in this paper .
to enable a direct comparison , dirichlet hyperpriors could also be placed on the hyperparameters of the priors described in section 3 .
conclusions .
creating a single model that integrates bigram-based and topic-based approaches to document modeling has several benefits .
firstly , the predictive accuracy of the new model , especially when using prior 2 , is significantly better than that of either latent dirichlet allocation or the hierarchical dirichlet language model .
secondly , the model automatically infers a separate topic for function words , meaning that the other topics are less dominated by these words .
