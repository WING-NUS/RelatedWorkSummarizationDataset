expressing implicit semantic relations without supervision .
abstract .
we present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .
for a given input word pair x : y with some unspecified semantic relations , the corresponding output list of patterns p1 , ... , pm is ranked according to how well each pattern pi expresses the relations between x and y. for example , given x = ostrich and y = bird , the two highest ranking output patterns are x is the largest y and y such as the x. the output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .
the patterns are sorted by pertinence , where the pertinence of a pattern pi for a word pair x : y is the expected relational similarity between the given pair and typical pairs for pi .
the algorithm is empirically evaluated on two tasks , solving multiple-choice sat word analogy questions and classifying semantic relations in noun-modifier pairs .
on both tasks , the algorithm achieves state- of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .
introduction .
in a widely cited paper , hearst ( 1992 ) showed that the lexico-syntactic pattern y such as the x can be used to mine large text corpora for word pairs x : y in which x is a hyponym ( type ) of y. for example , if we search in a large corpus using the pattern y such as the x and we find the string bird such as the ostrich , then we can infer that ostrich is a hyponym of bird .
berland and charniak ( 1999 ) demonstrated that the patterns ys x and x of the y can be used to mine corpora for pairs x : y in which x is a meronym ( part ) of y ( e.g. , wheel of the car ) .
here we consider the inverse of this problem : given a word pair x : y with some unspecified semantic relations , can we mine a large text corpus for lexico-syntactic patterns that express the implicit relations between x and y ?
for example , if we are given the pair ostrich : bird , can we discover the pattern y such as the x ?
we are particularly interested in discovering high quality patterns that are reliable for mining further word pairs with the same semantic relations .
in our experiments , we use a corpus of web pages containing about 5x1010 english words ( terra and clarke , 2003 ) .
from co-occurrences of the pair ostrich : bird in this corpus , we can generate 516 patterns of the form x ...
y and 452 patterns of the form y ...
x. most of these patterns are not very useful for text mining .
the main challenge is to find a way of ranking the patterns , so that patterns like y such as the x are highly ranked .
another challenge is to find a way to empirically evaluate the performance of any such pattern ranking algorithm .
for a given input word pair x : y with some unspecified semantic relations , we rank the corresponding output list of patterns p1 , ... , pm in order of decreasing pertinence .
the pertinence of a pattern pi for a word pair x : y is the expected relational similarity between the given pair and typical pairs that fit pi .
we define pertinence more precisely in section 2 .
hearst ( 1992 ) suggests that her work may be useful for building a thesaurus .
berland and charniak ( 1999 ) suggest their work may be useful for building a lexicon or ontology , like wordnet .
our algorithm is also applicable to these tasks .
other potential applications and related problems are discussed in section 3 .
to calculate pertinence , we must be able to measure relational similarity .
our measure is based on latent relational analysis ( turney , 2005 ) .
the details are given in section 4 .
given a word pair x : y , we want our algorithm to rank the corresponding list of patterns p1 , ... , pm according to their value for mining text , in support of semantic network construction and similar tasks .
unfortunately , it is difficult to measure performance on such tasks .
therefore our experiments are based on two tasks that provide objective performance measures .
in section 5 , ranking algorithms are compared by their performance on solving multiple-choice sat word analogy questions .
in section 6 , they are compared by their performance on classifying semantic relations in noun-modifier pairs .
the experiments demonstrate that ranking by pertinence is significantly better than several alternative pattern ranking algorithms , based on tf-idf .
the performance of pertinence on these two tasks is slightly below the best performance that has been reported so far ( turney , 2005 ) , but the difference is not statistically significant .
we discuss the results in section 7 and conclude in section 8 .
pertinence .
the relational similarity between two pairs of words , x1 : y1 and x2 : y2 , is the degree to which their semantic relations are analogous .
for example , mason : stone and carpenter : wood have a high degree of relational similarity .
measuring relational similarity will be discussed in section 4 .
for now , assume that we have a measure of the relational similarity between pairs of words , simr ( x1 : y1 , x2 : y2 ) .
pertinence tends to be highest with patterns that are unambiguous .
the maximum value of pertinence ( xj : yj , pi ) is attained when the pair xj : yj belongs to a cluster of highly similar pairs and the conditional probability distribution p ( xk : yk pi ) is concentrated on the cluster .
an ambiguous pattern , with its probability spread over multiple clusters , will have less pertinence .
if a pattern with high pertinence is used for text mining , it will tend to produce word pairs that are very similar to the given word pair ; this follows from the definition of pertinence .
we believe this definition is the first formal measure of quality for text mining patterns .
the use of bayes theorem and the assumption that p ( xj : yj ) = 1 n for all word pairs is a way of smoothing the probability lar to laplace smoothing .
related work .
lapata ( 2002 ) examines the task of expressing the implicit relations in nominalizations , which are noun compounds whose head noun is derived from a verb and whose modifier can be interpreted as an argument of the verb .
in contrast with this work , our algorithm is not restricted to nominalizations .
section 6 shows that our algorithm works with arbitrary noun compounds and the sat questions in section 5 include all nine possible pairings of nouns , verbs , and adjectives .
as far as we know , our algorithm is the first unsupervised learning algorithm that can find patterns for semantic relations , given only a large corpus ( e.g. , in our experiments , about 5 x 1010 words ) and a moderately sized set of word pairs ( e.g. , 600 or more pairs in the experiments ) , such that the members of each pair appear together frequently in short phrases in the corpus .
these word pairs are not seeds , since the algorithm does not require the pairs to be labeled or grouped ; we do not assume they are homogenous .
the word pairs that we need could be generated automatically , by searching for word pairs that co-occur frequently in the corpus .
however , our evaluation methods ( sections 5 and 6 ) both involve a predetermined list of word pairs .
if our algorithm were allowed to generate its own word pairs , the overlap with the predetermined lists would likely be small .
this is a limitation of our evaluation methods rather than the algorithm .
since any two word pairs may have some relations in common and some that are not shared , our algorithm generates a unique list of patterns for each input word pair .
turney ( 2005 ) gives an algorithm for measuring the relational similarity between two pairs of words , called latent relational analysis ( lra ) .
this algorithm can be used to solve multiple- choice word analogy questions and to classify noun-modifier pairs ( turney , 2005 ) , but it does not attempt to express the implicit semantic relations .
turney ( 2005 ) maps each pair x : y to a high-dimensional vector v ~ .
the value of each element vi in v ~ is based on the frequency , for the pair x : y , of a corresponding pattern pi .
the relational similarity between two pairs , x1 : y1 and x2 : y2 , is derived from the cosine of the angle between their two vectors .
a limitation of this approach is that the semantic content of the vectors is difficult to interpret ; the magnitude of an element vi is not a good indicator of how well the corresponding pattern pi expresses a relation of x : y. this claim is supported by the experiments in sections 5 and 6 .
pertinence ( as defined in section 2 ) builds on the measure of relational similarity in turney ( 2005 ) , but it has the advantage that the semantic content can be interpreted ; we can point to specific patterns and say that they express the implicit relations .
furthermore , we can use the patterns to find other pairs with the same relations .
hearst ( 1992 ) processed her text with a partof-speech tagger and a unification-based constituent analyzer .
this makes it possible to use more general patterns .
for example , instead of the literal string pattern y such as the x , where x and y are words , hearst ( 1992 ) used the more abstract pattern np0 such as np1 , where npi represents a noun phrase .
for the sake of simplicity , we have avoided part-of-speech tagging , which limits us to literal patterns .
we plan to experiment with tagging in future work .
the algorithm .
the algorithm takes as input a set of word pairs w = { x1 : y1 , ... , xn : yn } and produces as output ranked lists of patterns p1 , ... , pm for each input pair .
the following steps are similar to the algorithm of turney ( 2005 ) , with several changes to support the calculation of pertinence .
find phrases : for each pair xi : yi , make a list of phrases in the corpus that contain the pair .
we use the waterloo multitext system ( clarke et al. , 1998 ) to search in a corpus of about 1010 english words ( terra and clarke , 2003 ) .
make one list of phrases that begin with xi and end with yi and a second list for the opposite order .
each phrase must have one to three intervening words between xi and yi .
the first and last words in the phrase do not need to exactly match xi and yi .
the multitext query language allows different suffixes .
veale ( 2004 ) has observed that it is easier to identify semantic relations between nouns than between other parts of speech .
therefore we use wordnet 2.0 ( miller , 1995 ) to guess whether xi and yi are likely to be nouns .
when they are nouns , we are relatively strict about suffixes ; we only allow variation in pluralization .
for all other parts of speech , we are liberal about suffixes .
for example , we allow an adjective such as inflated to match a noun such as inflation .
with multitext , the query inflat * matches both inflated and inflation .
generate patterns : for each list of phrases , generate a list of patterns , based on the phrases .
replace the first word in each phrase with the generic marker x and replace the last word with y. the intervening words in each phrase may be either left as they are or replaced with the wildcard * .
for example , the phrase carpenter nails the wood yields the patterns x nails the y , x nails * y , x * the y , and x * * y. do not allow duplicate patterns in a list , but note the number of times a pattern is generated for each word pair xi : yi in each order ( xi first and yi last or vice versa ) .
we call this the pattern frequency .
it is a local frequency count , analogous to term frequency in information retrieval .
count pair frequency : the pair frequency for a pattern is the number of lists from the preceding step that contain the given pattern .
it is a global frequency count , analogous to document frequency in information retrieval .
note that a pair xi : yi yields two lists of phrases and hence two lists of patterns .
a given pattern might appear in zero , one , or two of the lists for xi : yi .
map pairs to rows : in preparation for building a matrix x , create a mapping of word pairs to row numbers .
for each pair xi : yi , create a row for xi : yi and another row for yi : xi .
if w does not already contain { y1 : x1 , ... , yn : xn } , then we have effectively doubled the number of word pairs , which increases the sample size for calculating pertinence .
map patterns to columns : create a mapping of patterns to column numbers .
for each unique pattern of the form x ...
y from step 2 , create a column for the original pattern x ...
y and another column for the same pattern with x and y swapped , y ...
x. step 2 can generate millions of distinct patterns .
the experiment in section 5 results in 1,706,845 distinct patterns , yielding 3,413,690 columns .
this is too many columns for matrix operations with todays standard desktop computer .
most of the patterns have a very low pair frequency .
for the experiment in section 5 , 1,371,702 of the patterns have a pair frequency of one .
to keep the matrix x manageable , we drop all patterns with a pair frequency less than ten .
for section 5 , this leaves 42,032 patterns , yielding 84,064 columns .
turney ( 2005 ) limited the matrix to 8,000 columns , but a larger pool of patterns is better for our purposes , since it increases the likelihood of finding good patterns for expressing the semantic relations of a given word pair .
build a sparse matrix : build a matrix x in sparse matrix format .
the value for the cell in row i and column j is the pattern frequency of the j-th pattern for the the i-th word pair .
calculate entropy : apply log and entropy transformations to the sparse matrix x ( landauer and dumais , 1997 ) .
each cell is replaced with its logarithm , multiplied by a weight based on the negative entropy of the corresponding column vector in the matrix .
this gives more weight to patterns that vary substantially in frequency for each pair .
apply svd : after log and entropy transforms , apply the singular value decomposition ( svd ) to x ( golub and van loan , 1996 ) .
svd decomposes x into a product of three matrices u ^ vt , where u and v are in column orthonormal form ( i.e. , the columns are orthogonal and have unit length ) and ^ is a diagonal matrix of singular values ( hence svd ) .
if x is of rank r , then ^ is also of rank r .
let ^ k , where k < r , be the diagonal matrix formed from the top k singular values , and let uk and vk be the matrices produced by selecting the corresponding columns from u and v .
the matrix uk ^ kvk is the matrix of rank k that best approximates the original matrix x , in the sense that it minimizes the approximation errors ( golub and van loan , 1996 ) .
following landauer and dumais ( 1997 ) , we use k = 300 .
we may think of this matrix ukekvk as a smoothed version of the original matrix .
svd is used to reduce noise and compensate for sparseness ( landauer and dumais , 1997 ) .
calculate cosines : the relational similarity between two pairs , simr ( x1 : y1 , x2 : y2 ) , is given by the cosine of the angle between their corresponding row vectors in the matrix uk ^ kvk ( turney , 2005 ) .
to calculate pertinence , we will need the relational similarity between all possible pairs of pairs .
all of the cosines can be efficiently derived from the matrix uk ^ k ( uk ^ k ) t ( landauer and dumais , 1997 ) .
calculate conditional probabilities : using bayes theorem ( see section 2 ) and the raw frequency data in the matrix x from step 6 , before log and entropy transforms , calculate the conditional probability p ( xi : yi pj ) for every row ( word pair ) and every column ( pattern ) .
ranking serves as a kind of normalization .
we have found that the relative rank of a pattern is more reliable as an indicator of its importance than the absolute pertinence .
this is analogous to information retrieval , where documents are ranked in order of their relevance to a query .
the relative rank of a document is more important than its actual numerical score ( which is usually hidden from the user of a search engine ) .
having two separate ranked lists helps to avoid bias .
for example , ostrich : bird generates 516 patterns of the form x ...
y and 452 patterns of the form y ...
x. since there are more patterns of the form x ...
y , there is a slight bias towards these patterns .
if the two lists were merged , the y ...
x patterns would be at a disadvantage .
experiments with word analogies in these experiments , we evaluate pertinence using 374 college-level multiple-choice word analogies , taken from the sat test .
for each question , there is a target word pair , called the stem pair , and five choice pairs .
the task is to find the choice that is most analogous ( i.e. , has the highest relational similarity ) to the stem .
this choice pair is called the solution and the other choices are distractors .
since there are six word pairs per question ( the stem and the five choices ) , there are 374 6 = 2244 pairs in the input set w. in step 4 of the algorithm , we double the pairs , but we also drop some pairs because they do not co-occur in the corpus .
this leaves us with 4194 rows in the matrix .
as mentioned in step 5 , the matrix has 84,064 columns ( patterns ) .
the sparse matrix density is 0.91 % .
to answer a sat question , we generate ranked lists of patterns for each of the six word pairs .
each choice is evaluated by taking the intersection of its patterns with the stems patterns .
the shared patterns are scored by the average of their rank in the stems lists and the choices lists .
since the lists are sorted in order of decreasing pertinence , a low score means a high pertinence .
our guess is the choice with the lowest scoring shared pattern .
table 2 shows the four highest ranking patterns for the stem and solution for the first example .
the pattern x lion y is anomalous , but the other patterns seem reasonable .
the shared pattern y such as the x is ranked 1 for both pairs , hence the average score for this pattern is 1.0 , as shown in table 1 .
note that the ostrich is the largest bird and lions are large cats , but the largest cat is the siberian tiger .
table 3 lists the top five pairs in w that match the pattern y such as the x. the pairs are sorted by p ( x : y p ) .
the pattern y such as the x is one of 146 patterns that are shared by ostrich : bird and lion : cat .
most of these shared patterns are not very informative .
in table 4 , we compare ranking patterns by pertinence to ranking by various other measures , mostly based on varieties of tf-idf ( term frequency times inverse document frequency , a common way to rank documents in information retrieval ) .
the tf-idf measures are taken from salton and buckley ( 1988 ) .
for comparison , we also include three algorithms that do not rank patterns ( the bottom three rows in the table ) .
these three algorithms can answer the sat questions , but they do not provide any kind of explanation for their answers .
all of the pattern ranking algorithms are given exactly the same sets of patterns to rank .
any differences in performance are due to the ranking method alone .
the algorithms may skip questions when the word pairs do not co-occur in the corpus .
all of the ranking algorithms skip the same set of 15 of the 374 sat questions .
precision is defined as the percentage of correct answers out of the questions that were answered ( not skipped ) .
recall is the percentage of correct answers out of the maximum possible number correct ( 374 ) .
the f measure is the harmonic mean of precision and recall .
for the tf-idf methods in table 4 , f is the pattern frequency , n is the pair frequency , f is the maximum f for all patterns for the given word pair , and n is the total number of word pairs .
by tf = f , idf = 1 / n , for example ( row 8 ) , we mean that f plays a role that is analogous to term frequency and 1 / n plays a role that is analogous to inverse document frequency .
that is , in row 8 , the patterns are ranked in decreasing order of pattern frequency divided by pair frequency .
table 4 also shows some ranking methods based on intermediate calculations in the algorithm in section 4 .
for example , row 2 in table 4 gives the results when patterns are ranked in order of decreasing values in the corresponding cells of the matrix x from step 7 .
row 12 in table 4 shows the results we would get using latent relational analysis ( turney , 2005 ) to rank patterns .
the results in row 12 support the claim made in section 3 , that lra is not suitable for ranking patterns , although it works well for answering the sat questions ( as we see in row 16 ) .
the vectors in lra yield a good measure of relational similarity , but the magnitude of the value of a specific element in a vector is not a good indicator of the quality of the corresponding pattern .
the best method for ranking patterns is pertinence ( row 1 in table 4 ) .
as a point of comparison , the performance of the average senior highschool student on the sat analogies is about 57 % ( turney and littman , 2005 ) .
the second best method is to use the values in the matrix x after the log and entropy transformations in step 7 ( row 2 ) .
the difference between these two methods is statistically significant with 95 % confidence .
pertinence ( row 1 ) performs slightly below latent relational analysis ( row 16 ; turney , 2005 ) , but the difference is not significant .
randomly guessing answers should yield an f of 20 % ( 1 out of 5 choices ) , but ranking patterns randomly ( row 13 ) results in an f of 26.4 % .
this is because the stem pair tends to share more patterns with the solution pair than with the distractors .
the minimum of a large set of random numbers is likely to be lower than the minimum of a small set of random numbers .
experiments with noun-modifiers .
in these experiments , we evaluate pertinence on the task of classifying noun-modifier pairs .
the problem is to classify a noun-modifier pair , such as flu virus , according to the semantic relation between the head noun ( virus ) and the modifier ( flu ) .
for example , flu virus is classified as a causality relation ( the flu is caused by a virus ) .
for these experiments , we use a set of 600 manually labeled noun-modifier pairs ( nastase and szpakowicz , 2003 ) .
there are five general classes of labels with thirty subclasses .
we present here the results with five classes ; the results with thirty subclasses follow the same trends ( that is , pertinence performs significantly better than the other ranking methods ) .
the five classes are causality ( storm cloud ) , temporality ( daily exercise ) , spatial ( desert storm ) , participant ( student protest ) , and quality ( expensive book ) .
the input set w consists of the 600 noun- modifier pairs .
this set is doubled in step 4 , but we drop some pairs because they do not co-occur in the corpus , leaving us with 1184 rows in the matrix .
there are 16,849 distinct patterns with a pair frequency of ten or more , resulting in 33,698 columns .
the matrix density is 2.57 % .
to classify a noun-modifier pair , we use a single nearest neighbour algorithm with leave-oneout cross-validation .
we split the set 600 times .
each pair gets a turn as the single testing example , while the other 599 pairs serve as training examples .
the testing example is classified according to the label of its nearest neighbour in the training set .
the distance between two noun- modifier pairs is measured by the average rank of their best shared pattern .
table 5 shows the resulting precision , recall , and f , when ranking patterns by pertinence .
to gain some insight into the algorithm , we examined the 600 best shared patterns for each pair and its single nearest neighbour .
for each of the five classes , table 6 lists the most frequent pattern among the best shared patterns for the given class .
all of these patterns seem appropriate for their respective classes .
table 7 gives the performance of pertinence on the noun-modifier problem , compared to various other pattern ranking methods .
the bottom two rows are included for comparison ; they are not pattern ranking algorithms .
the best method for ranking patterns is pertinence ( row 1 in table 7 ) .
the difference between pertinence and the second best ranking method ( row 2 ) is statistically significant with 95 % confidence .
latent relational analysis ( row 16 ) performs slightly better than pertinence ( row 1 ) , but the difference is not statistically significant .
row 6 in table 7 shows the results we would get using latent relational analysis ( turney , 2005 ) to rank patterns .
again , the results support the claim in section 3 , that lra is not suitable for ranking patterns .
lra can classify the noun- modifiers ( as we see in row 16 ) , but it cannot express the implicit semantic relations that make an unlabeled noun-modifier in the testing set similar to its nearest neighbour in the training set .
discussion .
computing pertinence took about 18 hours for the experiments in section 5 and 9 hours for section 6 .
in both cases , the majority of the time was spent in step 1 , using multitext ( clarke et al. , 1998 ) to search through the corpus of 5 x 1010 words .
multitext was running on a beowulf cluster with sixteen 2.4 ghz intel xeon cpus .
the corpus and the search index require about one terabyte of disk space .
this may seem computationally demanding by todays standards , but progress in hardware will soon allow an average desktop computer to handle corpora of this size .
although the performance on the sat analogy questions ( 54.6 % ) is near the level of the average senior highschool student ( 57 % ) , there is room for improvement .
for applications such as building a thesaurus , lexicon , or ontology , this level of performance suggests that our algorithm could assist , but not replace , a human expert .
one possible improvement would be to add part-of-speech tagging or parsing .
we have done some preliminary experiments with parsing and plan to explore tagging as well .
a difficulty is that much of the text in our corpus does not consist of properly formed sentences , since the text comes from web pages .
this poses problems for most part-of-speech taggers and parsers .
conclusion .
latent relational analysis ( turney , 2005 ) provides a way to measure the relational similarity between two word pairs , but it gives us little insight into how the two pairs are similar .
in effect , lra is a black box .
the main contribution of this paper is the idea of pertinence , which allows us to take an opaque measure of relational similarity and use it to find patterns that express the implicit semantic relations between two words .
the experiments in sections 5 and 6 show that ranking patterns by pertinence is superior to ranking them by a variety of tf-idf methods .
on the word analogy and noun-modifier tasks , pertinence performs as well as the state-of-the-art , lra , but pertinence goes beyond lra by making relations explicit .
